[{"path":"https://statistikat.github.io/taRantula/articles/Config.html","id":"the-configuration-hierarchy","dir":"Articles","previous_headings":"","what":"The Configuration Hierarchy","title":"Configuration Management with `params_manager`","text":"params_manager class follows specific priority loading settings: Defaults: Hardcoded values defined package. YAML File: Values loaded external file (overrides defaults). Manual Overrides: Values passed directly R (overrides everything).","code":""},{"path":"https://statistikat.github.io/taRantula/articles/Config.html","id":"using-the-configuration-for-a-scraping-job","dir":"Articles","previous_headings":"","what":"Using the Configuration for a scraping job","title":"Configuration Management with `params_manager`","text":"scraper configuration initialized using function paramsScraper() manages settings Selenium, robots.txt, directory paths.","code":""},{"path":"https://statistikat.github.io/taRantula/articles/Config.html","id":"initializing","dir":"Articles","previous_headings":"Using the Configuration for a scraping job","what":"Initializing","title":"Configuration Management with `params_manager`","text":"can initialize defaults override specific values immediately.","code":"library(taRantula)  # Basic initialization cfg <- paramsScraper(project = \"census_scrape\")  # Advanced initialization with nested Selenium settings cfg <- paramsScraper(   project = \"census_scrape\",   selenium = list(host = \"192.168.1.xx\", workers = 5) )"},{"path":"https://statistikat.github.io/taRantula/articles/Config.html","id":"getting-and-setting-values","dir":"Articles","previous_headings":"Using the Configuration for a scraping job","what":"Getting and Setting Values","title":"Configuration Management with `params_manager`","text":"class supports convenient $-syntax character vector syntax nested paths.","code":"# Accessing nested values cfg$get(\"selenium$host\") cfg$get(c(\"selenium\", \"port\"))  # Updating values (automatically triggers validation) cfg$set(\"selenium$port\", 4445) cfg$set(\"robots$check\", FALSE)"},{"path":"https://statistikat.github.io/taRantula/articles/Config.html","id":"using-the-configuration-for-a-google-custom-search","dir":"Articles","previous_headings":"","what":"Using the Configuration for a Google Custom Search","title":"Configuration Management with `params_manager`","text":"Google Search configuration can initialized function paramsGoogleSearch streamlined, focusing API limits result attributes.","code":"# Set up a Google Search task gcfg <- paramsGoogleSearch(   path = \"~/google_results\",   max_queries = 500,   scrape_attributes = c(\"link\", \"snippet\") # Only keep specific fields )  # Current state gcfg$show_config()"},{"path":"https://statistikat.github.io/taRantula/articles/Config.html","id":"portability-exporting-and-importing-yaml","dir":"Articles","previous_headings":"","what":"Portability: Exporting and Importing YAML","title":"Configuration Management with `params_manager`","text":"key feature reproducible research ability save configuration file. allows share exact scraper settings colleagues use CI/CD pipeline.","code":""},{"path":"https://statistikat.github.io/taRantula/articles/Config.html","id":"exporting","dir":"Articles","previous_headings":"Portability: Exporting and Importing YAML","what":"Exporting","title":"Configuration Management with `params_manager`","text":"","code":"# Save your current configuration cfg$export(\"my_config.yaml\")  # You can also save just the package defaults as a template cfg$write_defaults(\"template.yaml\")"},{"path":"https://statistikat.github.io/taRantula/articles/Config.html","id":"importing","dir":"Articles","previous_headings":"Portability: Exporting and Importing YAML","what":"Importing","title":"Configuration Management with `params_manager`","text":"","code":"# Recreate a scraper state from a YAML file new_cfg <- paramsScraper(config_file = \"my_config.yaml\")"},{"path":"https://statistikat.github.io/taRantula/articles/Config.html","id":"built-in-validation","dir":"Articles","previous_headings":"","what":"Built-in Validation","title":"Configuration Management with `params_manager`","text":"params_manager provides strict type range checking. attempt set invalid value, package throw informative error immediately.","code":"# This will trigger an error (port must be an integerish number <= 65535) try(cfg$set(\"selenium$port\", 99999))  # This will trigger an error (scrape_attributes must be one of the allowed fields) try(gcfg$set(\"scrape_attributes\", \"raw_html\"))"},{"path":[]},{"path":"https://statistikat.github.io/taRantula/articles/Intro.html","id":"prerequisites-selenium-grid","dir":"Articles","previous_headings":"","what":"Prerequisites: Selenium Grid","title":"Getting Started with taRantula","text":"scraping, need running Selenium Grid. Using Docker recommended easisest way manage browser overhead outside R.","code":""},{"path":"https://statistikat.github.io/taRantula/articles/Intro.html","id":"docker-compose-setup","dir":"Articles","previous_headings":"Prerequisites: Selenium Grid","what":"Docker-Compose Setup","title":"Getting Started with taRantula","text":"Save following docker-compose.yaml run docker-compose -d. configuration optimized memory demands modern browsers. Note: Create network first needed: can achieved using command docker network create net-selenium-grid.","code":"version: \"3\" services:   hub:   container_name: selenium-hub image: selenium/hub:4.8 ports:   - 4442-4444:4442-4444 networks:   - net-selenium-grid environment:   - NODE_MAX_INSTANCES=3   - NODE_MAX_SESSION=3  node-chrome:   image: selenium/node-chrome:138.0   container_name: selenium-grid-chrome   depends_on:     - hub   networks:     - net-selenium-grid   environment:     - SE_EVENT_BUS_HOST=hub     - SE_EVENT_BUS_PUBLISH_PORT=4442     - SE_EVENT_BUS_SUBSCRIBE_PORT=4443     - SE_NODE_OVERRIDE_MAX_SESSIONS=true     - SE_NODE_MAX_SESSIONS=3     - SE_ENABLE_BROWSER_LEFTOVERS_CLEANUP=true     - SE_BROWSER_LEFTOVERS_INTERVAL_SECS=600     - SE_BROWSER_LEFTOVERS_TEMPFILES_DAYS=2   shm_size: 2g  networks:   net-selenium-grid:   external: true"},{"path":"https://statistikat.github.io/taRantula/articles/Intro.html","id":"basic-workflow","dir":"Articles","previous_headings":"","what":"Basic Workflow","title":"Getting Started with taRantula","text":"package contains following processes: define google custom search parameters via R6-based configuration run google custom search. define scraping parameters via R6-based configuration initializing scraper.","code":""},{"path":[]},{"path":"https://statistikat.github.io/taRantula/articles/Intro.html","id":"configuration","dir":"Articles","previous_headings":"Google custom search","what":"Configuration","title":"Getting Started with taRantula","text":"Set following environmental variables: SCRAPING_APIKEY_GOOGLE google custom search api key SCRAPING_ENGINE_GOOGLE google custom search engine ID Use paramsGoogleSearch() manage settings. ensures nested parameters validated google custom search starts.","code":""},{"path":"https://statistikat.github.io/taRantula/articles/Intro.html","id":"custom-search","dir":"Articles","previous_headings":"Google custom search","what":"Custom search","title":"Getting Started with taRantula","text":"Run google custom search api queries build enterprise name addresses.","code":"Sys.setenv(   SCRAPING_APIKEY_GOOGLE = \"My_ApiKey\",   SCRAPING_ENGINE_GOOGLE = \"My_Engine\" )  cfg <- paramsGoogleSearch(   path = \"~/path/to/my/project\",   scrape_attributes = c(\"title\", \"link\", \"displayLink\", \"snippet\") )  dat <- data.table(   ID = c(1, 2, 3),   EnterpriseName = c(\"Name1\", \"Name2\", \"Name3\"),   EnterpriseAddress = c(\"Address1\", \"Address2\", \"Address3\") )  # build queries dat[, Query1 := buildQuery(.SD), .SDcols = c(   \"EnterpriseName\",   \"EnterpriseAddress\" )] dat[, Query2 := buildQuery(.SD), .SDcols = c(\"EnterpriseAddress\")]  # update keys in config cfg$set(key = \"query_col\", c(\"Query1\", \"Query2\")) cfg$set(key = \"file\", c(\"File1.csv\", \"File2.csv\")) # <- files to save results in  # run google custom search runGoogleSearch(   cfg = cfg,   data = data )"},{"path":[]},{"path":"https://statistikat.github.io/taRantula/articles/Intro.html","id":"configuration-1","dir":"Articles","previous_headings":"Scraping","what":"Configuration","title":"Getting Started with taRantula","text":"Use paramsScraper() manage settings. ensures nested parameters validated scraper starts. UrlScraper class manages state, connection DuckDB database, parallel worker pool.","code":"library(taRantula)  # Initialize scraping parameters cfg <- paramsScraper() cfg$set(\"selenium$host\", \"localhost\") cfg$set(\"selenium$workers\", 3) cfg$set(\"urls\", c(\"https://nytimes.com\", \"https://whitehouse.gov\"))  # Initialize the scraper s <- UrlScraper$new(cfg)  # Start scraping s$scrape()"},{"path":"https://statistikat.github.io/taRantula/articles/Intro.html","id":"scraping-engines-selenium-vs--httr","dir":"Articles","previous_headings":"Scraping","what":"Scraping Engines: Selenium vs. httr","title":"Getting Started with taRantula","text":"scrape() implementation engine-agnostic. detects whether session full browser set HTTP headers adjusts strategy accordingly.","code":""},{"path":"https://statistikat.github.io/taRantula/articles/Intro.html","id":"selenium-mode-javascript-support","dir":"Articles","previous_headings":"Scraping > Scraping Engines: Selenium vs. httr","what":"Selenium Mode (JavaScript Support)","title":"Getting Started with taRantula","text":"default, scraper expects Selenium Grid work . necessary modern websites require JavaScript render content. Redirect Detection: scraper automatically detects URL redirects (e.g., http https) stores url_redirect pointer database. Content Persistence: full rendered HTML source captured DOM stabilized.","code":""},{"path":"https://statistikat.github.io/taRantula/articles/Intro.html","id":"httr-fallback-high-speed","dir":"Articles","previous_headings":"Scraping > Scraping Engines: Selenium vs. httr","what":"httr Fallback (High Speed)","title":"Getting Started with taRantula","text":"disable Selenium, scraper falls back httr::GET(). significantly faster uses fewer resources allow catch dynamically rendered content. use: static sites APIs JavaScript rendering required. Custom Headers: mode, scraper passes configured user-agent metadata via HTTP headers.","code":"# To disable Selenium and use the httr fallback: cfg$set(\"selenium$use_selenium\", FALSE)  # The same UrlScraper$new(cfg) will now use httr internally"},{"path":"https://statistikat.github.io/taRantula/articles/Intro.html","id":"fault-tolerance-and-progress-protection","dir":"Articles","previous_headings":"Scraping","what":"Fault Tolerance and Progress Protection","title":"Getting Started with taRantula","text":"One core strengths taRantula resilience. Scraping thousands URLs prone interruptions (network timeouts, hardware crashes, OS updates).","code":""},{"path":"https://statistikat.github.io/taRantula/articles/Intro.html","id":"snapshotting-mechanism","dir":"Articles","previous_headings":"Scraping > Fault Tolerance and Progress Protection","what":"Snapshotting Mechanism","title":"Getting Started with taRantula","text":"configuration includes snapshot_every parameter (default 10). works: Every N URLs, worker writes current progress temporary “snapshot” file project directory. Crash Recovery: R session Selenium server crashes, simply re-initialize UrlScraper configuration. startup, class detects snapshots existing DuckDB database. Persistence: URLs processed since last snapshot lost, ensuring 99% progress usually preserved.","code":""},{"path":"https://statistikat.github.io/taRantula/articles/Intro.html","id":"data-extraction-and-iteration","dir":"Articles","previous_headings":"Scraping","what":"Data Extraction and Iteration","title":"Getting Started with taRantula","text":"Data persisted DuckDB, meaning can access via helper methods raw SQL.","code":"# Access results as a data.table results_dt <- s$results()  # Access all discovered links links_dt <- s$links()  # Inspect logs for errors or redirects logs_dt <- s$logs()"},{"path":"https://statistikat.github.io/taRantula/articles/Intro.html","id":"advanced-extraction-regex","dir":"Articles","previous_headings":"Scraping > Data Extraction and Iteration","what":"Advanced Extraction: Regex","title":"Getting Started with taRantula","text":"can also perform targeted data mining across results. example, find Austrian UID (VAT number) pages likely “Imprints”:","code":"# Pattern for Austrian VAT numbers uid_pattern <- \"ATU[0-9]{8}\"  # Extract only from pages where the URL or the link label matches 'imprint' vat_results <- s$regex_extract(   pattern = uid_pattern,   filter_links = \"imprint\",   ignore_cases = TRUE )"},{"path":[]},{"path":"https://statistikat.github.io/taRantula/articles/Intro.html","id":"adding-new-urls","dir":"Articles","previous_headings":"Scraping > Controlling the Scraper","what":"Adding New URLs","title":"Getting Started with taRantula","text":"can feed newly discovered links back scraper. taRantula automatically handles duplicate detection.","code":"# Add new URLs and scrape again s$update_urls(urls = c(\"https://google.com\", \"https://www.statistik.at\")) s$scrape()"},{"path":"https://statistikat.github.io/taRantula/articles/Intro.html","id":"graceful-termination","dir":"Articles","previous_headings":"Scraping > Controlling the Scraper","what":"Graceful Termination","title":"Getting Started with taRantula","text":"need stop long-running scrape, use $stop(). creates signal file tells workers finish current URL exit cleanly.","code":"s$stop()"},{"path":"https://statistikat.github.io/taRantula/articles/Intro.html","id":"cleanup","dir":"Articles","previous_headings":"Scraping > Controlling the Scraper","what":"Cleanup","title":"Getting Started with taRantula","text":"Always close scraper shut DuckDB connection clean temporary files.","code":"s$close()"},{"path":"https://statistikat.github.io/taRantula/articles/Intro.html","id":"resources","dir":"Articles","previous_headings":"","what":"Resources","title":"Getting Started with taRantula","text":"Google Custom Search API Docker Engine Installation Selenium Grid Documentation","code":""},{"path":"https://statistikat.github.io/taRantula/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Johannes Gussenbauer. Author, maintainer. Bernhard Meindl. Author.","code":""},{"path":"https://statistikat.github.io/taRantula/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Gussenbauer J, Meindl B (2026). taRantula: taRantula package provides methods utility-functions scrape web pages. R package version 0.1.0, https://statistikat.github.io/taRantula.","code":"@Manual{,   title = {taRantula: The taRantula package provides methods and utility-functions to scrape web pages},   author = {Johannes Gussenbauer and Bernhard Meindl},   year = {2026},   note = {R package version 0.1.0},   url = {https://statistikat.github.io/taRantula}, }"},{"path":"https://statistikat.github.io/taRantula/index.html","id":"tarantula","dir":"","previous_headings":"","what":"The taRantula package provides methods and utility-functions to scrape web pages","title":"The taRantula package provides methods and utility-functions to scrape web pages","text":"taRantula R package designed robust, large-scale web scraping. combines flexibility Selenium speed httr, backed persistent DuckDB storage engine ensure data integrity.","code":""},{"path":"https://statistikat.github.io/taRantula/index.html","id":"key-features","dir":"","previous_headings":"","what":"Key Features","title":"The taRantula package provides methods and utility-functions to scrape web pages","text":"Hybrid Scraping Engine: Seamlessly switch Selenium 4 (JS-heavy sites) httr (high-speed static content). Persistent Storage: results written directly DuckDB backend, allowing SQL-based querying zero data loss. Selenium Grid Ready: Optimized containerized Hub/Node architectures high-memory environments. Fault Tolerance: Features snapshotting mechanism resume interrupted jobs last stable state. Parallel Processing: Scales across multiple workers using future framework. Regex Data Mining: High-performance extraction emails, VAT/UID numbers, custom patterns directly collected data.","code":""},{"path":"https://statistikat.github.io/taRantula/index.html","id":"configuration-params_manager","dir":"","previous_headings":"","what":"Configuration (params_manager)","title":"The taRantula package provides methods and utility-functions to scrape web pages","text":"package uses robust, R6-based configuration system strict type validation: paramsScraper(): General web crawling JS rendering settings. paramsGoogleSearch(): Specialized config Google Search API rate-limit handling. YAML Support: Easily export import configurations reproducible scraping pipelines.","code":""},{"path":"https://statistikat.github.io/taRantula/index.html","id":"compliance-and-safety","dir":"","previous_headings":"","what":"Compliance and Safety","title":"The taRantula package provides methods and utility-functions to scrape web pages","text":"Robots.txt Enforcement: Automated checking internal caching respect site owner preferences. Graceful Termination: Signaling mechanisms ensure workers exit cleanly without corrupting database. Redirect Detection: Logs tracks URL changes request final browser state.","code":""},{"path":"https://statistikat.github.io/taRantula/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"The taRantula package provides methods and utility-functions to scrape web pages","text":"","code":"# Install from GitHub remotes::install_github(\"statistikat/taRantula\")"},{"path":"https://statistikat.github.io/taRantula/index.html","id":"quick-start","dir":"","previous_headings":"","what":"Quick Start","title":"The taRantula package provides methods and utility-functions to scrape web pages","text":"basic example initialize scraping job using Selenium engine DuckDB storage. advanced users looking run containerized environment, please refer Intro Vignette: Docker-based Selenium Setup.","code":"library(taRantula)  # 1. Setup Configuration cfg <- paramsScraper() cfg$set(\"selenium$host\", \"localhost\") cfg$set(\"selenium$port\", 4444L) cfg$set(\"storage$path\", \"scraping_results.duckdb\")  # 2. Initialize the Scraper scraper <- UrlScraper$new(config = cfg)  # 3. Define URLs and Run urls <- c(\"[https://example.com](https://example.com)\", \"[https://r-project.org](https://r-project.org)\") scraper$run(urls)  # 4. Extract Data (e.g., Email addresses) emails <- scraper$regex_extract(pattern = \"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\")  # 5. Graceful Stop scraper$stop()"},{"path":"https://statistikat.github.io/taRantula/index.html","id":"production-deployment","dir":"","previous_headings":"","what":"Production Deployment","title":"The taRantula package provides methods and utility-functions to scrape web pages","text":"production environments, package includes docker-compose templates spin Selenium Grid alongside R environment. Detailed instructions available documentation vignettes.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":null,"dir":"Reference","previous_headings":"","what":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"UrlScraper R6 class provides high‑level framework scraping list URLs using multiple parallel Selenium (non‑Selenium) workers. manages scraping state, progress, snapshots, logs, respects robots.txt rules. Results logs stored internal DuckDB database.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"R6 class generator class UrlScraper.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"overview","dir":"Reference","previous_headings":"","what":"Overview","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"UrlScraper class designed robust, resumable web scraping workflows. key features include: Parallel scraping URLs via multiple Selenium workers Persistent storage results, logs, extracted links DuckDB Automatic snapshotting recovery partially processed chunks Respecting robots.txt rules via pre‑checks domains Convenience helpers querying results, logs, extracted links Regex‑based extraction text previously scraped HTML","code":""},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"configuration","dir":"Reference","previous_headings":"","what":"Configuration","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"configuration object (typically created via paramsScraper) expected contain least following entries: db_file – path DuckDB database file snapshot_dir – directory temporary snapshot files progress_dir – directory progress/log files stop_file – path file used signal workers stop urls / urls_todo – vectors URLs URLs still scrape selenium – list Selenium‑related settings, : use_selenium – logical, whether use Selenium workers – number parallel Selenium workers host, port, browser, verbose – Selenium connection settings ecaps – list Chrome options (args, prefs, excludeSwitches) snapshot_every – number URLs snapshot taken robots – list robots.txt handling options, : check – logical, whether check robots.txt snapshot_every – snapshot frequency robots checks workers – number workers robots.txt checks robots_user_agent – user agent string used robots queries exclude_social_links – logical, whether exclude social media links exact structure depends paramsScraper related helpers.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"initialize(config) – create new UrlScraper instance scrape() – scrape remaining URLs parallel update_urls(urls, force = FALSE) – add new URLs queue results(filter = NULL) – extract scraping results logs(filter = NULL) – extract log entries links(filter = NULL) – extract discovered links query(q) – run custom SQL queries internal DuckDB database regex_extract(pattern, group = NULL, filter_links = NULL, ignore_cases = TRUE) – extract text via regex scraped HTML stop() – create stop‑file workers can exit gracefully close() – clean snapshots close database connections","code":""},{"path":[]},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"UrlScraper$new() UrlScraper$scrape() UrlScraper$update_urls() UrlScraper$results() UrlScraper$logs() UrlScraper$links() UrlScraper$query() UrlScraper$regex_extract() UrlScraper$stop() UrlScraper$close() UrlScraper$clone()","code":""},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"Create new UrlScraper object. constructor initializes internal storage (DuckDB database, snapshot progress directories), restores previous snapshots/logs present, configures progress handlers.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"","code":"UrlScraper$new(config)"},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"config list (configuration object) settings, typically created paramsScraper(). include: db_file – path DuckDB database file. snapshot_dir – directory snapshot files. progress_dir – directory progress/log files. stop_file – path stop signal file. urls_todo – character vector URLs still scraped. selenium – list Selenium settings (host, port, workers, etc.). robots – list robots.txt handling options. additional options required helper functions.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"initialized UrlScraper object (invisibly).","code":""},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"method-scrape-","dir":"Reference","previous_headings":"","what":"Method scrape()","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"Scrape remaining URLs using parallel workers.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"","code":"UrlScraper$scrape()"},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"method orchestrates parallel scraping process: Re‑initializes storage processes existing snapshots logs. Computes set URLs still scrape. Optionally performs robots.txt checks new domains. Sets parallel plan via future framework. Starts multiple Selenium (non‑Selenium) sessions. Distributes URLs across workers tracks global progress. Cleans snapshots/logs updates internal URL state scraping. stop‑file detected (see stop()), scraping aborted starting. Workers also honor stop‑file terminate gracefully finishing current URL.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"UrlScraper object (invisibly), internal state updated reflect newly scraped URLs.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"method-update-urls-","dir":"Reference","previous_headings":"","what":"Method update_urls()","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"Update list URLs scraped.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"","code":"UrlScraper$update_urls(urls, force = FALSE)"},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"urls character vector new URLs add. force logical flag. TRUE, given URLs kept except duplicates within urls (check already scraped URLs). FALSE (default), URLs already database duplicates urls removed.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"details-1","dir":"Reference","previous_headings":"","what":"Details","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"method updates internal URL queue based given input vector urls. Depending force: force = FALSE (default), URLs already scraped (.e. present results database) removed, well duplicates within urls vector . force = TRUE, duplicates within given urls vector removed; URLs already present database kept. Summary information many URLs added, already known, duplicates printed via cli.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"UrlScraper object (invisibly).","code":""},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"method-results-","dir":"Reference","previous_headings":"","what":"Method results()","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"Extract scraping results internal database.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"","code":"UrlScraper$results(filter = NULL)"},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"filter Optional character string SQL‑like condition (without keyword), e.g. \"url LIKE 'https://example.com/%'\". NULL (default), rows results table returned.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"returns-3","dir":"Reference","previous_headings":"","what":"Returns","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"data.table containing scraping results.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"method-logs-","dir":"Reference","previous_headings":"","what":"Method logs()","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"Extract log entries internal database.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"","code":"UrlScraper$logs(filter = NULL)"},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"filter Optional character string SQL‑like condition (without keyword). NULL (default), rows logs table returned.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"returns-4","dir":"Reference","previous_headings":"","what":"Returns","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"data.table containing log entries.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"method-links-","dir":"Reference","previous_headings":"","what":"Method links()","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"Extract scraped links internal database.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"","code":"UrlScraper$links(filter = NULL)"},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"filter Optional character string SQL‑like condition (without keyword). NULL (default), rows links table returned.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"returns-5","dir":"Reference","previous_headings":"","what":"Returns","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"data.table containing extracted links.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"method-query-","dir":"Reference","previous_headings":"","what":"Method query()","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"Execute custom SQL query internal DuckDB database. low‑level helper advanced use cases. assumes user familiar schema internal database (tables results, logs, links, others created helper functions).","code":""},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"","code":"UrlScraper$query(q)"},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"arguments-5","dir":"Reference","previous_headings":"","what":"Arguments","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"q character string containing valid DuckDB SQL query.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"returns-6","dir":"Reference","previous_headings":"","what":"Returns","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"result query, typically data.table.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"method-regex-extract-","dir":"Reference","previous_headings":"","what":"Method regex_extract()","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"Extract text scraped HTML using regular expression.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"","code":"UrlScraper$regex_extract(   pattern,   group = NULL,   filter_links = NULL,   ignore_cases = TRUE )"},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"arguments-6","dir":"Reference","previous_headings":"","what":"Arguments","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"pattern character string containing regular expression. Named capture groups supported. group Either: character string naming capture group (e.g. \"name\" pattern contains (?<name>...)), integer specifying index capture group return. NULL (default), behavior delegated .extract_regex() may return groups depending implementation. filter_links character vector containing keywords partial words used filter set URLs pattern extracted. example, filter_links = \"imprint\" restricts extraction URLs whose href label contains \"imprint\". ignore_cases Logical. TRUE (default), case ignored matching pattern. FALSE, pattern matched case‑sensitive way.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"details-2","dir":"Reference","previous_headings":"","what":"Details","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"helper performs post‑processing step stored HTML sources results table: first selects links links table whose href label match provided filter_links terms. identifies documents (rows results) whose url among selected links status == TRUE. Finally, applies regular expression HTML source documents returns extracted matches. particularly useful extracting structured information email addresses, phone numbers, IDs subset pages (e.g. contact imprint pages).","code":""},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"returns-7","dir":"Reference","previous_headings":"","what":"Returns","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"data.table (similar object) returned .extract_regex(), typically containing matched text corresponding URLs.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"method-stop-","dir":"Reference","previous_headings":"","what":"Method stop()","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"Create stop‑file signal running workers terminate gracefully.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"usage-8","dir":"Reference","previous_headings":"","what":"Usage","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"","code":"UrlScraper$stop()"},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"details-3","dir":"Reference","previous_headings":"","what":"Details","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"Workers periodically check existence configured stop_file. present, finish processing current URL exit. allows controlled shutdown long‑running scraping job without abruptly terminating R session Selenium instances.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"returns-8","dir":"Reference","previous_headings":"","what":"Returns","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"Invisible NULL.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"method-close-","dir":"Reference","previous_headings":"","what":"Method close()","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"Clean resources, including snapshots database connections.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"usage-9","dir":"Reference","previous_headings":"","what":"Usage","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"","code":"UrlScraper$close()"},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"details-4","dir":"Reference","previous_headings":"","what":"Details","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"method performs following clean‑steps: Processes remaining snapshots logs. Deletes snapshot directory (exists). Opens DuckDB connection configured db_file disconnects shutdown = TRUE. good practice call close() done UrlScraper instance.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"returns-9","dir":"Reference","previous_headings":"","what":"Returns","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"Invisible NULL.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"objects class cloneable method.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"usage-10","dir":"Reference","previous_headings":"","what":"Usage","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"","code":"UrlScraper$clone(deep = FALSE)"},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"arguments-7","dir":"Reference","previous_headings":"","what":"Arguments","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"deep Whether make deep clone.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/UrlScraper.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper","text":"","code":"if (FALSE) { # \\dontrun{ # Create a default configuration object cfg <- paramsScraper()  # Example Selenium settings cfg$set(\"selenium$host\", \"localhost\") cfg$set(\"selenium$workers\", 2) cfg$show_config()  # Initialize the scraper scraper <- UrlScraper$new(config = cfg)  # Start scraping remaining URLs scraper$scrape()  # Retrieve results as a data.table results_dt <- scraper$results()  # Retrieve logs and links logs_dt  <- scraper$logs() links_dt <- scraper$links()  # Add new URLs to be scraped (only those not already in the DB) scraper$update_urls(urls = c(\"https://example.com/\"))  # Force adding URLs (ignores duplicates against already scraped ones) scraper$update_urls(urls = c(\"https://example.com/\"), force = TRUE)  # Regex extraction from scraped HTML emails_dt <- scraper$regex_extract(   pattern      = \"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Za-z]{2,}\",   filter_links = c(\"contact\", \"imprint\") )  # Stop ongoing workers after they finish the current URL scraper$stop()  # Clean up resources scraper$close() } # }"},{"path":"https://statistikat.github.io/taRantula/reference/buildQuery.html","id":null,"dir":"Reference","previous_headings":"","what":"Build Encoded Search Queries — buildQuery","title":"Build Encoded Search Queries — buildQuery","text":"Constructs URL‑encoded search queries selected columns data frame data table. function typically used prepare query strings Google Custom Search API.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/buildQuery.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Build Encoded Search Queries — buildQuery","text":"","code":"buildQuery(dat, selectCols = NULL)"},{"path":"https://statistikat.github.io/taRantula/reference/buildQuery.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Build Encoded Search Queries — buildQuery","text":"dat data.frame data.table containing text variables used assemble search string. selectCols Character vector column names include query. NULL, columns dat used.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/buildQuery.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Build Encoded Search Queries — buildQuery","text":"character vector encoded search queries. returned object includes attribute \"query_attr\" = \"built_encoded_query\" used downstream checks.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/buildQuery.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Build Encoded Search Queries — buildQuery","text":"","code":"## Example usage will be added later"},{"path":"https://statistikat.github.io/taRantula/reference/check_links.html","id":null,"dir":"Reference","previous_headings":"","what":"Link Validation Helper — check_links","title":"Link Validation Helper — check_links","text":"Evaluates extracted URLs determines retained processing. function filters links : belong domain baseurl Point files images, audio, video, archives, executables, etc. Refer fragments anchor points Refer back path main page","code":""},{"path":"https://statistikat.github.io/taRantula/reference/check_links.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Link Validation Helper — check_links","text":"","code":"check_links(hrefs, baseurl)"},{"path":"https://statistikat.github.io/taRantula/reference/check_links.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Link Validation Helper — check_links","text":"hrefs Character vector URLs check. baseurl Character string giving original page URL domain path comparison.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/check_links.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Link Validation Helper — check_links","text":"logical vector indicating entries hrefs retained.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/check_robotsdata.html","id":null,"dir":"Reference","previous_headings":"","what":"Check Whether a URL Is Allowed According to Stored robots.txt Rules — check_robotsdata","title":"Check Whether a URL Is Allowed According to Stored robots.txt Rules — check_robotsdata","text":"Determines whether URL permitted scraped according robots.txt rules stored DuckDB \"robots\" table.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/check_robotsdata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Check Whether a URL Is Allowed According to Stored robots.txt Rules — check_robotsdata","text":"","code":"check_robotsdata(db_file, url)"},{"path":"https://statistikat.github.io/taRantula/reference/check_robotsdata.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Check Whether a URL Is Allowed According to Stored robots.txt Rules — check_robotsdata","text":"db_file character(1) Path DuckDB database file. url character(1) URL evaluate.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/check_robotsdata.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Check Whether a URL Is Allowed According to Stored robots.txt Rules — check_robotsdata","text":"TRUE scraping URL allowed, FALSE otherwise.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/check_robotsdata.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Check Whether a URL Is Allowed According to Stored robots.txt Rules — check_robotsdata","text":"Internally calls query_robotsdata() evaluates permissions via robotstxt::paths_allowed(). valid robots.txt information available domain, function returns TRUE (.e., scraping allowed).","code":""},{"path":[]},{"path":"https://statistikat.github.io/taRantula/reference/check_robotsdata.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Check Whether a URL Is Allowed According to Stored robots.txt Rules — check_robotsdata","text":"","code":"if (FALSE) { # \\dontrun{ check_robotsdata(\"robots.duckdb\", \"https://example.com/secret\") } # }"},{"path":"https://statistikat.github.io/taRantula/reference/dot-default_useragent.html","id":null,"dir":"Reference","previous_headings":"","what":"Default User‑Agent String — .default_useragent","title":"Default User‑Agent String — .default_useragent","text":"Provides default desktop Safari‑style user‑agent string Selenium httr requests custom value supplied.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-default_useragent.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Default User‑Agent String — .default_useragent","text":"","code":".default_useragent()"},{"path":"https://statistikat.github.io/taRantula/reference/dot-default_useragent.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Default User‑Agent String — .default_useragent","text":"character scalar representing valid browser user‑agent.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-default_useragent.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Default User‑Agent String — .default_useragent","text":"user‑agent string chosen mimic typical macOS Safari browser environment reduce likelihood blocked websites using automated scraping tools.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-extract_query.html","id":null,"dir":"Reference","previous_headings":"","what":"Execute Arbitrary SQL Query on DuckDB — .extract_query","title":"Execute Arbitrary SQL Query on DuckDB — .extract_query","text":"Executes custom SQL query DuckDB database used scraper. function provides maximum flexibility advanced users need run specialized SQL statements beyond standard table extractors.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-extract_query.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Execute Arbitrary SQL Query on DuckDB — .extract_query","text":"","code":".extract_query(db_file, query)"},{"path":"https://statistikat.github.io/taRantula/reference/dot-extract_query.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Execute Arbitrary SQL Query on DuckDB — .extract_query","text":"db_file Path DuckDB database file. query Character scalar containing valid SQL query.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-extract_query.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Execute Arbitrary SQL Query on DuckDB — .extract_query","text":"data.table containing retrieved results, NULL invisibly query fails.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-extract_query.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Execute Arbitrary SQL Query on DuckDB — .extract_query","text":"function: Validates DuckDB file exists Executes provided SQL read‑mode Converts result data.table Returns NULL invisibly query fails low‑level function intended power users. Users must ensure SQL queries syntactically valid.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-extract_query.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Execute Arbitrary SQL Query on DuckDB — .extract_query","text":"","code":"if (FALSE) { # \\dontrun{ ## List all domains stored in robots table: .extract_query(\"results.duckdb\", \"SELECT domain FROM robots\")  ## Count pages scraped successfully: .extract_query(\"results.duckdb\",                \"SELECT COUNT(*) FROM results WHERE status = TRUE\") } # }"},{"path":"https://statistikat.github.io/taRantula/reference/dot-extract_regex.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Regular Expression Matches from Scraped HTML — .extract_regex","title":"Extract Regular Expression Matches from Scraped HTML — .extract_regex","text":"Applies regular expression previously scraped HTML documents, optionally restricted specific capture group. document first cleaned using parse_HTML() remove non‑text content, ensuring reliable pattern extraction.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-extract_regex.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Regular Expression Matches from Scraped HTML — .extract_regex","text":"","code":".extract_regex(docs, urls, pattern, group = NULL, ignore_cases = TRUE)"},{"path":"https://statistikat.github.io/taRantula/reference/dot-extract_regex.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Regular Expression Matches from Scraped HTML — .extract_regex","text":"docs Character vector list HTML source documents. urls Character vector URLs corresponding docs. pattern regular expression search . group Optional capture group name index extract. NULL, full match returned. ignore_cases Logical; TRUE, performs case‑insensitive matching.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-extract_regex.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract Regular Expression Matches from Scraped HTML — .extract_regex","text":"data.table row corresponds match includes: url – originating document URL pattern (given group name) – Extracted values Missing matches returned NA_character_.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-extract_regex.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract Regular Expression Matches from Scraped HTML — .extract_regex","text":"function: Cleans normalizes HTML document Converts text lowercase ignore_cases = TRUE Extracts regex matches using stringr::str_match_all() Supports named numbered capture groups Returns unified data.table indexed URL Named groups allow meaningful column labeling result.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-extract_regex.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract Regular Expression Matches from Scraped HTML — .extract_regex","text":"","code":"if (FALSE) { # \\dontrun{ ## Extract email-like patterns: .extract_regex(docs, urls, pattern = \"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\") } # }"},{"path":"https://statistikat.github.io/taRantula/reference/dot-extract_results.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Table Results from DuckDB — .extract_results","title":"Extract Table Results from DuckDB — .extract_results","text":"Retrieves records one internal DuckDB tables used UrlScraper framework. Supported tables include: \"results\" – scraped HTML documents \"logs\" – worker progress log entries \"links\" – extracted hyperlinks Optional SQL-style filtering supported (e.g., \"url LIKE 'https://example.com/%'\").","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-extract_results.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Table Results from DuckDB — .extract_results","text":"","code":".extract_results(db_file, tab = \"results\", filter)"},{"path":"https://statistikat.github.io/taRantula/reference/dot-extract_results.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Table Results from DuckDB — .extract_results","text":"db_file Path DuckDB file created scraper. tab Character scalar specifying table query. Must one \"results\", \"logs\", \"links\". filter Optional SQL clause (without word ) used subset results.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-extract_results.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract Table Results from DuckDB — .extract_results","text":"data.table containing rows selected table, optionally filtered. Returns NULL invisibly query fails.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-extract_results.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract Table Results from DuckDB — .extract_results","text":"helper function: Connects DuckDB database read‑mode Validates requested table name Constructs SELECT * <table> query, optionally clause Returns results data.table underlying query fails (often due malformed filters), informative message printed NULL returned invisibly.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-extract_results.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract Table Results from DuckDB — .extract_results","text":"","code":"if (FALSE) { # \\dontrun{ ## Extract all scraped results: .extract_results(\"results.duckdb\", tab = \"results\")  ## Extract links from a specific domain: .extract_results(\"results.duckdb\", tab = \"links\",                   filter = \"href LIKE 'https://example.com/%'\") } # }"},{"path":"https://statistikat.github.io/taRantula/reference/dot-filter_new_urls.html","id":null,"dir":"Reference","previous_headings":"","what":"Filter New URLs by Removing Duplicates and Already-Scraped URLs — .filter_new_urls","title":"Filter New URLs by Removing Duplicates and Already-Scraped URLs — .filter_new_urls","text":"function filters vector newly collected URLs excluding already scraped appear new input. URL parsing performed using urltools, duplicates detected normalization URLs.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-filter_new_urls.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Filter New URLs by Removing Duplicates and Already-Scraped URLs — .filter_new_urls","text":"","code":".filter_new_urls(urls_scraped = NULL, urls_new, return_index = FALSE)"},{"path":"https://statistikat.github.io/taRantula/reference/dot-filter_new_urls.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Filter New URLs by Removing Duplicates and Already-Scraped URLs — .filter_new_urls","text":"urls_scraped Optional list vector containing URLs already scraped. NULL, internal duplicates urls_new removed. urls_new Character vector new URLs filtered. return_index Logical; TRUE, function returns indices duplicates previously scraped URLs instead filtered URLs.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-filter_new_urls.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Filter New URLs by Removing Duplicates and Already-Scraped URLs — .filter_new_urls","text":"return_index = FALSE (default): character vector filtered URLs. return_index = TRUE: list elements index_old: indices URLs found urls_scraped index_duplicate: indices duplicated URLs within urls_new","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-filter_new_urls.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Filter New URLs by Removing Duplicates and Already-Scraped URLs — .filter_new_urls","text":"URL parsing handled internally via helper function extracts URL components, removes scheme field, attaches domain. Matching new previously scraped URLs done using data.table's keyed joins efficiently identify overlaps.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-filter_new_urls.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Filter New URLs by Removing Duplicates and Already-Scraped URLs — .filter_new_urls","text":"","code":"if (FALSE) { # \\dontrun{ # Example URLs new_urls <- c(\"https://example.com\", \"https://example.com/page\",               \"https://example.com\")  old_urls <- c(\"https://example.com/page\")  # Filter new URLs filtered <- .filter_new_urls(urls_scraped = old_urls, urls_new = new_urls)  # Inspect indices of removed URLs idx <- .filter_new_urls(urls_scraped = old_urls, urls_new = new_urls,                         return_index = TRUE) } # }"},{"path":"https://statistikat.github.io/taRantula/reference/dot-fmt.html","id":null,"dir":"Reference","previous_headings":"","what":"Format Numeric Values — .fmt","title":"Format Numeric Values — .fmt","text":"Simple wrapper around formatC() used internally preparing numeric output (e.g., percentages durations) scraper messages.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-fmt.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Format Numeric Values — .fmt","text":"","code":".fmt(x, format = \"f\", digits = 3)"},{"path":"https://statistikat.github.io/taRantula/reference/dot-fmt.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Format Numeric Values — .fmt","text":"x numeric value. format Character string indicating desired output format; passed formatC(). Defaults \"f\". digits Number digits decimal point. Defaults 3.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-fmt.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Format Numeric Values — .fmt","text":"character string formatted numeric output.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-get_scraped_urls.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve Scraped URLs from a DuckDB Database — .get_scraped_urls","title":"Retrieve Scraped URLs from a DuckDB Database — .get_scraped_urls","text":"function reads previously scraped URLs DuckDB database file. returns data frame containing original URLs corresponding redirect URLs stored results table.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-get_scraped_urls.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve Scraped URLs from a DuckDB Database — .get_scraped_urls","text":"","code":".get_scraped_urls(db_file)"},{"path":"https://statistikat.github.io/taRantula/reference/dot-get_scraped_urls.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve Scraped URLs from a DuckDB Database — .get_scraped_urls","text":"db_file Character string specifying path DuckDB database file. file exist, NULL returned.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-get_scraped_urls.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve Scraped URLs from a DuckDB Database — .get_scraped_urls","text":"NULL database file exist. empty character vector table results available. data frame columns url url_redirect otherwise.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-get_scraped_urls.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Retrieve Scraped URLs from a DuckDB Database — .get_scraped_urls","text":"function safely opens DuckDB database read‑mode ensures connection properly closed upon exit. table results queried. table missing, error thrown.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-get_scraped_urls.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Retrieve Scraped URLs from a DuckDB Database — .get_scraped_urls","text":"","code":"if (FALSE) { # \\dontrun{ # Load previously scraped URLs scraped <- .get_scraped_urls(\"my_scraper_db.duckdb\") } # }"},{"path":"https://statistikat.github.io/taRantula/reference/dot-handle_logs.html","id":null,"dir":"Reference","previous_headings":"","what":"Import and Store Scraping Log Files — .handle_logs","title":"Import and Store Scraping Log Files — .handle_logs","text":"Reads individual progress log files generated scraping, parses contents, inserts collected entries logs table DuckDB results database. successful insertion, processed log files removed filesystem.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-handle_logs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Import and Store Scraping Log Files — .handle_logs","text":"","code":".handle_logs(progress_dir, db_file)"},{"path":"https://statistikat.github.io/taRantula/reference/dot-handle_logs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Import and Store Scraping Log Files — .handle_logs","text":"progress_dir Path directory containing log files produced scraping. db_file Path DuckDB database file logs stored.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-handle_logs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Import and Store Scraping Log Files — .handle_logs","text":"Invisibly returns TRUE logs imported (possible) corresponding files removed.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-handle_logs.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Import and Store Scraping Log Files — .handle_logs","text":"function performs following actions: Scans progress_dir log files created parallel scraper workers Parses log file line‑‑line, splitting entries : timestamp chunk/work‑unit identifier URL currently processed Converts parsed entries data frame suitable database storage Inserts log entries DuckDB logs table using \"INSERT IGNORE\" avoid duplicates Removes successfully processed log files Log files expected contain tab‑separated entries created worker processes. Files empty unreadable automatically discarded.","code":""},{"path":[]},{"path":"https://statistikat.github.io/taRantula/reference/dot-handle_robots.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve and Store robots.txt Information in a DuckDB Database — .handle_robots","title":"Retrieve and Store robots.txt Information in a DuckDB Database — .handle_robots","text":"Retrieves robots.txt files set domains, parses permissions, stores DuckDB table named \"robots\". Existing entries overwritten. Domains valid robots.txt can retrieved stored empty permissions, implying fully permissive access.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-handle_robots.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve and Store robots.txt Information in a DuckDB Database — .handle_robots","text":"","code":".handle_robots(db_file, snapshot_every, workers, urls, user_agent = NULL)"},{"path":"https://statistikat.github.io/taRantula/reference/dot-handle_robots.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve and Store robots.txt Information in a DuckDB Database — .handle_robots","text":"db_file character(1) Path DuckDB database file. snapshot_every integer(1) Number domains process per chunk. workers integer(1) Number worker processes used parallel retrieval. urls character Vector URLs corresponding domains extracted. user_agent character(1) Optional user agent string passed robotstxt::robotstxt().","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-handle_robots.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve and Store robots.txt Information in a DuckDB Database — .handle_robots","text":"Invisibly returns NULL. Side effect: updates (creates) table \"robots\" supplied DuckDB file.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-handle_robots.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Retrieve and Store robots.txt Information in a DuckDB Database — .handle_robots","text":"function processes domains parallel, retrieves robots.txt rules, stores chunks improve efficiency. automatically detects domains already present database processes missing ones. stored permissions can later queried using query_robotsdata().","code":""},{"path":[]},{"path":"https://statistikat.github.io/taRantula/reference/dot-handle_robots.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Retrieve and Store robots.txt Information in a DuckDB Database — .handle_robots","text":"","code":"if (FALSE) { # \\dontrun{ db <- \"robots.duckdb\" urls <- c(\"https://example.com\", \"https://r-project.org\") .handle_robots(db_file = db, snapshot_every = 10, workers = 2, urls = urls) } # }"},{"path":"https://statistikat.github.io/taRantula/reference/dot-handle_snapshots.html","id":null,"dir":"Reference","previous_headings":"","what":"Handle and Import Snapshot Files into the DuckDB Database — .handle_snapshots","title":"Handle and Import Snapshot Files into the DuckDB Database — .handle_snapshots","text":"Processes snapshot .rds files found given directory, extracts scraped content discovered hyperlinks, writes associated DuckDB database.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-handle_snapshots.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Handle and Import Snapshot Files into the DuckDB Database — .handle_snapshots","text":"","code":".handle_snapshots(snapshot_dir, db_file)"},{"path":"https://statistikat.github.io/taRantula/reference/dot-handle_snapshots.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Handle and Import Snapshot Files into the DuckDB Database — .handle_snapshots","text":"snapshot_dir character(1) Path directory containing snapshot .rds files. files matching snap_*.rds (recursively) processed. db_file character(1) Path DuckDB database file. Must already exist.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-handle_snapshots.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Handle and Import Snapshot Files into the DuckDB Database — .handle_snapshots","text":"invisible(TRUE) success, invisible(NULL) snapshots exist. errors database insertion, function prints diagnostic message leaves snapshot files untouched.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-handle_snapshots.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Handle and Import Snapshot Files into the DuckDB Database — .handle_snapshots","text":"function designed use snapshot‑based web‑scraping workflow: snapshot contains scraped page data (content) extracted hyperlinks (links). function: Reads pending snapshot files Normalizes merges content link tables Inserts/updates records DuckDB tables results links Ensures hierarchical link levels respected Removes snapshot files successful processing function side‑effect heavy: performs database writes, link‑level conflict resolution, deletes files processed. Snapshot files expected contain list least two elements: content: data.table holding scraped page data links: list link records, convertible data.table links table must contain least: href — Discovered link label — Link label source_url — URL link extracted scraped_at — timestamp scraping Link levels assigned follows: Level 1 previously unseen base URLs Otherwise, max(existing level) + 1 Updates use INSERT ... CONFLICT (...) UPDATE, proposed new level lower existing one (.e., \"shorter path\").","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-handle_snapshots.html","id":"database-requirements","dir":"Reference","previous_headings":"","what":"Database Requirements","title":"Handle and Import Snapshot Files into the DuckDB Database — .handle_snapshots","text":"DuckDB database must contain following tables: results compatible columns matching batch_content links columns href, label, source_url, level, scraped_at","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-handle_snapshots.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Handle and Import Snapshot Files into the DuckDB Database — .handle_snapshots","text":"","code":"if (FALSE) { # \\dontrun{ # Directory containing snapshot files dir <- \"snapshots/\"  # Existing DuckDB database file db <- \"scraper_results.duckdb\"  # Process and import all snapshots .handle_snapshots(snapshot_dir = dir, db_file = db) } # }"},{"path":"https://statistikat.github.io/taRantula/reference/dot-init_storage.html","id":null,"dir":"Reference","previous_headings":"","what":"Initialize DuckDB Storage Structure — .init_storage","title":"Initialize DuckDB Storage Structure — .init_storage","text":"Creates internal DuckDB storage schema yet exist. Required directories created tables results, links, logs, robots‑permissions initialized.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-init_storage.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Initialize DuckDB Storage Structure — .init_storage","text":"","code":".init_storage(db_file, snapshot_dir, progress_dir)"},{"path":"https://statistikat.github.io/taRantula/reference/dot-init_storage.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Initialize DuckDB Storage Structure — .init_storage","text":"db_file Path DuckDB database file. snapshot_dir Directory intermediate snapshots stored. progress_dir Directory incremental progress logs.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-init_storage.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Initialize DuckDB Storage Structure — .init_storage","text":"Invisibly returns TRUE ensuring storage ready.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-init_storage.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Initialize DuckDB Storage Structure — .init_storage","text":"DuckDB database already exists, function performs destructive actions simply returns. Otherwise, : Connects DuckDB file Creates four tables (already present): results – scraped pages status, HTML, timestamps links – extracted hyperlinks labels metadata logs – progress tracking entries robots – stored robots.txt permissions visited domains table definitions include primary keys ensure data integrity.","code":""},{"path":[]},{"path":"https://statistikat.github.io/taRantula/reference/dot-initialize.html","id":null,"dir":"Reference","previous_headings":"","what":"Initialize Scraper Configuration — .initialize","title":"Initialize Scraper Configuration — .initialize","text":"Internal utility prepares normalizes configuration list used UrlScraper class. includes creating required directories, setting file paths, determining pending URLs, applying global options needed scraping.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-initialize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Initialize Scraper Configuration — .initialize","text":"","code":".initialize(config)"},{"path":"https://statistikat.github.io/taRantula/reference/dot-initialize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Initialize Scraper Configuration — .initialize","text":"config cfg_scraper configuration object.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-initialize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Initialize Scraper Configuration — .initialize","text":"normalized configuration list ready use scraping engine.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-initialize.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Initialize Scraper Configuration — .initialize","text":"function performs following steps: Validates provided configuration object cfg_scraper instance Constructs project directory base_dir Creates required subfolders snapshots, progress files, DuckDB database Initializes URL queue (urls_todo) marks none scraped initially existing DuckDB file found, previously scraped URLs loaded removed queue Stores current global R options applies scraper‑specific defaults function called automatically inside UrlScraper constructor used directly.","code":""},{"path":[]},{"path":"https://statistikat.github.io/taRantula/reference/dot-scrape_single_url.html","id":null,"dir":"Reference","previous_headings":"","what":"Scrape a Single URL — .scrape_single_url","title":"Scrape a Single URL — .scrape_single_url","text":"function retrieves processes content single URL using either Selenium session HTTP request. extracts HTML source, identifies potential redirects, parses links page, returns structured data.table containing scraping results.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-scrape_single_url.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Scrape a Single URL — .scrape_single_url","text":"","code":".scrape_single_url(db_file, sid, url, robots_check)"},{"path":"https://statistikat.github.io/taRantula/reference/dot-scrape_single_url.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Scrape a Single URL — .scrape_single_url","text":"db_file Character string specifying path DuckDB database file used robots.txt rule evaluation. sid Either Selenium session object (SeleniumSession) named list HTTP headers used httr::GET(). url Character string containing URL scraped. robots_check Logical indicating whether robots.txt rules validated scraping.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-scrape_single_url.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Scrape a Single URL — .scrape_single_url","text":"data.table following columns: url Final URL potential redirection. url_redirect Original URL, redirect occurred; otherwise NA. status Logical indicating whether scraping succeeded. src HTML source (NA scraping failed disallowed). links list-column containing extracted link information data.table. scraped_at POSIXct timestamp indicating scrape occurred.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-scrape_single_url.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Scrape a Single URL — .scrape_single_url","text":"function first checks robots.txt rules using check_robotsdata(). scraping disallowed, standardized record returned. using Selenium, browser navigated URL potentially redirected final URL captured. non-Selenium inputs, HTTP GET request performed. Errors scraping caught converted structured output.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-worker_scrape.html","id":null,"dir":"Reference","previous_headings":"","what":"Worker Function for Batched URL Scraping — .worker_scrape","title":"Worker Function for Batched URL Scraping — .worker_scrape","text":"internal function orchestrates scraping multiple URLs parallel processing contexts. manages progress logging, snapshot creation, robots.txt validation, stopping conditions.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-worker_scrape.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Worker Function for Batched URL Scraping — .worker_scrape","text":"","code":".worker_scrape(inputs, sid)"},{"path":"https://statistikat.github.io/taRantula/reference/dot-worker_scrape.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Worker Function for Batched URL Scraping — .worker_scrape","text":"inputs named list containing: db_file Path DuckDB file used robots.txt checks. urls Character vector URLs process worker. chunk_id Numeric identifier worker chunk. snapshot_every Integer: write snapshot files every N URLs. snapshot_dir Directory snapshot output stored. stop_file Path file whose existence indicates scraping stop early. progress_dir Directory storing progress logs. robots_check Logical indicating whether robots.txt rules evaluated. p progress callback function accepting arguments amount message. sid Selenium session object list HTTP headers, passed along .scrape_single_url().","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-worker_scrape.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Worker Function for Batched URL Scraping — .worker_scrape","text":"Invisibly returns TRUE completing scraping tasks assigned worker.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-worker_scrape.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Worker Function for Batched URL Scraping — .worker_scrape","text":"function iterates provided URLs, invoking .scrape_single_url() . Progress logged file, optional snapshot files store intermediate results safeguard worker interruptions. stop file detected, worker terminates early. remaining un-snapshotted results written end execution.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-worker_scrape.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Worker Function for Batched URL Scraping — .worker_scrape","text":"","code":"if (FALSE) { # \\dontrun{ # Inside a parallel worker .worker_scrape(   inputs = list(     db_file = \"mydb.duckdb\",     urls = c(\"https://example1.com\", \"https://example2.com\"),     chunk_id = 1,     snapshot_every = 50,     snapshot_dir = \"snapshots/\",     stop_file = \"stop.flag\",     progress_dir = \"progress/\",     robots_check = TRUE,     p = function(amount, message) cat(amount, message, \"\\n\")   ),   sid = my_selenium_session ) } # }"},{"path":"https://statistikat.github.io/taRantula/reference/dot-write_snapshot.html","id":null,"dir":"Reference","previous_headings":"","what":"Write Snapshot File to Disk — .write_snapshot","title":"Write Snapshot File to Disk — .write_snapshot","text":"Saves data snapshot (data.table) specified snapshot directory, naming chunk ID timestamp. typically called batched scraping operations persist intermediate results.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-write_snapshot.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Write Snapshot File to Disk — .write_snapshot","text":"","code":".write_snapshot(dt, chunk_id, snapshot_dir)"},{"path":"https://statistikat.github.io/taRantula/reference/dot-write_snapshot.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Write Snapshot File to Disk — .write_snapshot","text":"dt data.table containing scraped data extracted links. chunk_id integer(1) Identifier current chunk. Used file naming snap_chunkXX_*. snapshot_dir character(1) Directory snapshot file written.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-write_snapshot.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Write Snapshot File to Disk — .write_snapshot","text":"invisible(NULL). function writes .rds file side effect.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/dot-write_snapshot.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Write Snapshot File to Disk — .write_snapshot","text":"","code":"if (FALSE) { # \\dontrun{ dt <- data.table::data.table(   url = \"https://example.com\",   scraped_at = Sys.time(),   html = \"<html><\/html>\" )  .write_snapshot(dt, chunk_id = 1, snapshot_dir = \"snapshots/\") } # }"},{"path":"https://statistikat.github.io/taRantula/reference/extractLinks.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Hyperlinks from an HTML Document — extractLinks","title":"Extract Hyperlinks from an HTML Document — extractLinks","text":"Extracts valid hyperlinks HTML document returns cleaned normalized data.table. function parses <>, <area>, <base>, <link> elements, resolves relative URLs, removes invalid unwanted links, enriches output metadata source URL, extraction level, timestamp.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/extractLinks.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Hyperlinks from an HTML Document — extractLinks","text":"","code":"extractLinks(doc, baseurl)"},{"path":"https://statistikat.github.io/taRantula/reference/extractLinks.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Hyperlinks from an HTML Document — extractLinks","text":"doc character string containing HTML xml_document object. baseurl Character string representing URL document originated. Used resolve relative links filter domains.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/extractLinks.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract Hyperlinks from an HTML Document — extractLinks","text":"data.table containing following columns: href – Cleaned validated absolute URLs label – Link text extracted anchor element source_url – originating page links extracted level – Extraction depth (always 0 function) scraped_at – Timestamp extraction Duplicate URLs automatically removed.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/extractLinks.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract Hyperlinks from an HTML Document — extractLinks","text":"extractor designed web‑scraping pipelines meaningful, navigable hyperlinks desired. function: Converts inputs XML document necessary Extracts link text normalizes whitespace Resolves relative URLs provided baseurl Forces URLs use https:// Removes invalid links using check_links() Ensures uniqueness extracted links","code":""},{"path":"https://statistikat.github.io/taRantula/reference/extractLinks.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract Hyperlinks from an HTML Document — extractLinks","text":"","code":"html <- \"<html><body><a href='/about'>About<\/a><\/body><\/html>\" extractLinks(html, baseurl = \"https://example.com\") #>                         href  label          source_url level #>                       <char> <char>              <char> <num> #> 1: https://example.com/about  About https://example.com     0 #>             scraped_at #>                 <POSc> #> 1: 2026-01-22 12:41:20"},{"path":"https://statistikat.github.io/taRantula/reference/getGoogleCreds.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve Google Search Credentials — getGoogleCreds","title":"Retrieve Google Search Credentials — getGoogleCreds","text":"Reads Google Custom Search API credentials environment variables. allows secure decoupling API keys code.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/getGoogleCreds.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve Google Search Credentials — getGoogleCreds","text":"","code":"getGoogleCreds()"},{"path":"https://statistikat.github.io/taRantula/reference/getGoogleCreds.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve Google Search Credentials — getGoogleCreds","text":"named list elements: engine – Google Custom Search Engine ID key – API key string","code":""},{"path":"https://statistikat.github.io/taRantula/reference/getGoogleCreds.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Retrieve Google Search Credentials — getGoogleCreds","text":"following environment variables must defined: SCRAPING_APIKEY_GOOGLE – Google Custom Search API key SCRAPING_ENGINE_GOOGLE – Custom Search Engine (CSE) identifier values can defined inside ~/.Renviron set runtime using Sys.setenv().","code":""},{"path":"https://statistikat.github.io/taRantula/reference/getGoogleCreds.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Retrieve Google Search Credentials — getGoogleCreds","text":"","code":"## Example: Sys.setenv(SCRAPING_APIKEY_GOOGLE = \"your_key\") Sys.setenv(SCRAPING_ENGINE_GOOGLE = \"your_engine\") creds <- getGoogleCreds() print(creds) #> $engine #> [1] \"your_engine\" #>  #> $key #> [1] \"your_key\" #>"},{"path":"https://statistikat.github.io/taRantula/reference/get_domain.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Domain from URLs — get_domain","title":"Extract Domain from URLs — get_domain","text":"Extracts domain portion URLs optionally includes scheme (http:// https://). function removes common subdomains www. consistency.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/get_domain.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Domain from URLs — get_domain","text":"","code":"get_domain(x, include_scheme = FALSE)"},{"path":"https://statistikat.github.io/taRantula/reference/get_domain.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Domain from URLs — get_domain","text":"x Character vector URLs. include_scheme Logical; TRUE, prepend detected scheme returned domain.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/get_domain.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract Domain from URLs — get_domain","text":"character vector containing domain names. URLs parsed return original input value.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/paramsGoogleSearch.html","id":null,"dir":"Reference","previous_headings":"","what":"Google Search Configuration Class — cfg_googlesearch","title":"Google Search Configuration Class — cfg_googlesearch","text":"cfg_googlesearch R6 class inherits params_manager provides configuration management performing Google Custom Search API queries. handles: Definition default parameters Google search jobs YAML-based configuration overrides Programmatic overrides via ... Validation relevant configuration fields utility function simplifies creation cfg_googlesearch object. supports optional configuration file loading programmatic overrides.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/paramsGoogleSearch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Google Search Configuration Class — cfg_googlesearch","text":"","code":"paramsGoogleSearch(config_file = NULL, path = tempdir(), ...)"},{"path":"https://statistikat.github.io/taRantula/reference/paramsGoogleSearch.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Google Search Configuration Class — cfg_googlesearch","text":"R6::R6Class generator object.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/paramsGoogleSearch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Google Search Configuration Class — cfg_googlesearch","text":"config_file Optional path YAML configuration file. path Path directory used storing downloaded data. Defaults tempdir(). ... Additional named configuration overrides. take precedence defaults YAML configuration.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/paramsGoogleSearch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Google Search Configuration Class — cfg_googlesearch","text":"cfg_googlesearch object.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/paramsGoogleSearch.html","id":"key-features","dir":"Reference","previous_headings":"","what":"Key Features","title":"Google Search Configuration Class — cfg_googlesearch","text":"Built-defaults suitable scraping workflows Support API credentials (key + engine ID) Control query frequency batching Control metadata fields keep API responses Optional saving results disk","code":""},{"path":"https://statistikat.github.io/taRantula/reference/paramsGoogleSearch.html","id":"super-class","dir":"Reference","previous_headings":"","what":"Super class","title":"Google Search Configuration Class — cfg_googlesearch","text":"taRantula::params_manager -> cfg_googlesearch","code":""},{"path":"https://statistikat.github.io/taRantula/reference/paramsGoogleSearch.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Google Search Configuration Class — cfg_googlesearch","text":"taRantula::params_manager$export() taRantula::params_manager$get() taRantula::params_manager$load_config() taRantula::params_manager$print() taRantula::params_manager$set() taRantula::params_manager$show_config() taRantula::params_manager$update() taRantula::params_manager$write_defaults()","code":""},{"path":"https://statistikat.github.io/taRantula/reference/paramsGoogleSearch.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Google Search Configuration Class — cfg_googlesearch","text":"cfg_googlesearch$new() cfg_googlesearch$defaults() cfg_googlesearch$clone()","code":""},{"path":"https://statistikat.github.io/taRantula/reference/paramsGoogleSearch.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Google Search Configuration Class — cfg_googlesearch","text":"Initialize new cfg_googlesearch configuration object. load precedence : Defaults YAML configuration file (provided) Programmatic overrides via ...","code":""},{"path":"https://statistikat.github.io/taRantula/reference/paramsGoogleSearch.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Google Search Configuration Class — cfg_googlesearch","text":"","code":"cfg_googlesearch$new(config_file = NULL, path = tempdir(), ...)"},{"path":"https://statistikat.github.io/taRantula/reference/paramsGoogleSearch.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Google Search Configuration Class — cfg_googlesearch","text":"config_file (Optional) Path YAML configuration file. Supported settings include: path: Directory output data stored. id_col: Column name serving unique identifier entity (default: \"kz_z\"). query_col: Column name containing search queries. Must created via buildQuery() (default: NULL). print_every_n: Positive integer. Interval displaying progress messages (default: 100). save_every_n: Positive integer. Interval saving intermediate results (default: 500). scrape_attributes: Character vector. Specifies data extract results. One : \"title\", \"link\", \"displayLink\", \"snippet\" (default: c(\"link\", \"displayLink\")). verbose: Logical. progress updates printed console? (default: TRUE). max_queries: Maximum queries allowed per 24-hour period. reached, process pause 24-hour window resets (default: 10000). max_query_rate: Numeric. Maximum number queries allowed per 100 seconds (default: 100). file: Filename (relative path) saving results. NULL, results written disk. Uses data.table::fwrite() internally. overwrite: Logical. TRUE, existing files overwritten. FALSE, existing data loaded via data.table::fread() new results appended. Ensure column names match appending (default: FALSE). credentials: named list containing Google API credentials. Use \"key\" \"SCRAPING_APIKEY_GOOGLE\" API Key, \"engine\" \"SCRAPING_ENGINE_GOOGLE\" Search Engine ID. omitted, environment variables used. See also getGoogleCreds(). path Path directory project data stored. Overrides path setting config_file. ... Named arguments used override specific configuration settings. take precedence config_file default values.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/paramsGoogleSearch.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Google Search Configuration Class — cfg_googlesearch","text":"configured object class cfg_googlesearch.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/paramsGoogleSearch.html","id":"method-defaults-","dir":"Reference","previous_headings":"","what":"Method defaults()","title":"Google Search Configuration Class — cfg_googlesearch","text":"Return default configuration settings Google Custom Search.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/paramsGoogleSearch.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Google Search Configuration Class — cfg_googlesearch","text":"","code":"cfg_googlesearch$defaults()"},{"path":"https://statistikat.github.io/taRantula/reference/paramsGoogleSearch.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Google Search Configuration Class — cfg_googlesearch","text":"named list containing default values : path – directory store output id_col – identifier column query_col – column containing query strings print_every_n – progress message interval save_every_n – save interval scrape_attributes – CSE fields keep verbose – print progress messages max_queries – maximum queries per 24h max_query_rate – queries per 100 seconds file – output file (NULL) overwrite – overwrite output file append credentials – list key engine","code":""},{"path":"https://statistikat.github.io/taRantula/reference/paramsGoogleSearch.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Google Search Configuration Class — cfg_googlesearch","text":"objects class cloneable method.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/paramsGoogleSearch.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Google Search Configuration Class — cfg_googlesearch","text":"","code":"cfg_googlesearch$clone(deep = FALSE)"},{"path":"https://statistikat.github.io/taRantula/reference/paramsGoogleSearch.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Google Search Configuration Class — cfg_googlesearch","text":"deep Whether make deep clone.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/paramsGoogleSearch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Google Search Configuration Class — cfg_googlesearch","text":"","code":"# Create with defaults # in this case, Environment-Variables `SCRAPING_APIKEY_GOOGLE` and `SCRAPING_ENGINE_GOOGLE` # need to be set beforehand cfg <- paramsGoogleSearch() #> ℹ No configuration file provided for 'cfg_googlesearch'. Using default configuration.  # Create with overrides cfg <- paramsGoogleSearch(   path = getwd(),   credentials = list(     key = \"my_google_apikey\",     engine = \"my-search-engine-id\"   ),   verbose = FALSE ) #> Set/Updated environment variables for SCRAPING_APIKEY_GOOGLE and SCRAPING_ENGINE_GOOGLE. #> ℹ No configuration file provided for 'cfg_googlesearch'. Using default configuration.  # Return the current configuration cfg$show_config() #> $path #> [1] \"/tmp/RtmppVykHU/file1b7447f41c24/reference\" #>  #> $id_col #> [1] \"ID\" #>  #> $query_col #> NULL #>  #> $print_every_n #> [1] 100 #>  #> $save_every_n #> [1] 500 #>  #> $scrape_attributes #> [1] \"link\"        \"displayLink\" #>  #> $verbose #> [1] FALSE #>  #> $max_queries #> [1] 10000 #>  #> $max_query_rate #> [1] 100 #>  #> $file #> NULL #>  #> $overwrite #> [1] FALSE #>  #> $credentials #> $credentials$engine #> [1] \"my-search-engine-id\" #>  #> $credentials$key #> [1] \"my_google_apikey\" #>  #>   # Write current configuration to file f <- file.path(tempdir(), \"config.yaml\") cfg$export(f) #> ℹ Current configuration for 'cfg_googlesearch' written to '/tmp/RtmppVykHU/config.yaml'.  # Load from exported config-file and override cfg <- paramsGoogleSearch(config_file = f, verbose = TRUE) #> ℹ Configuration loaded from '/tmp/RtmppVykHU/config.yaml' for cfg_googlesearch try(file.remove(f)) #> [1] TRUE  # Return the current configuration cfg$show_config() #> $path #> [1] \"/tmp/RtmppVykHU\" #>  #> $id_col #> [1] \"ID\" #>  #> $print_every_n #> [1] 100 #>  #> $save_every_n #> [1] 500 #>  #> $scrape_attributes #> [1] \"link\"        \"displayLink\" #>  #> $verbose #> [1] TRUE #>  #> $max_queries #> [1] 10000 #>  #> $max_query_rate #> [1] 100 #>  #> $overwrite #> [1] FALSE #>  #> $credentials #> $credentials$engine #> [1] \"my-search-engine-id\" #>  #> $credentials$key #> [1] \"my_google_apikey\" #>  #>   # Or a specific setting cfg$get(\"max_query_rate\") #> [1] 100  # Update the configuration cfg$set(\"max_query_rate\", 200) cfg$get(\"max_query_rate\") #> [1] 200"},{"path":"https://statistikat.github.io/taRantula/reference/paramsScraper.html","id":null,"dir":"Reference","previous_headings":"","what":"Scraper Configuration Class — cfg_scraper","title":"Scraper Configuration Class — cfg_scraper","text":"cfg_scraper R6 configuration class inherits params_manager provides structured way manage configuration parameters generic web scraper. designed Selenium-based scraping workflows integrated robots.txt checks also supports httr::GET()-based scraping. Convenience constructor creating cfg_scraper object. supports optional YAML-based configuration programmatic overrides. recommended entry point users want configure scraping projects without interacting R6 class API directly.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/paramsScraper.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Scraper Configuration Class — cfg_scraper","text":"","code":"paramsScraper(config_file = NULL, base_dir = getwd(), ...)"},{"path":"https://statistikat.github.io/taRantula/reference/paramsScraper.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Scraper Configuration Class — cfg_scraper","text":"R6::R6Class generator object.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/paramsScraper.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Scraper Configuration Class — cfg_scraper","text":"config_file Optional path YAML configuration file. base_dir Path folder project data stored. Defaults getwd(). ... Named arguments override specific configuration settings (see initialize() method cfg_scraper details).","code":""},{"path":"https://statistikat.github.io/taRantula/reference/paramsScraper.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Scraper Configuration Class — cfg_scraper","text":"cfg_scraper object.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/paramsScraper.html","id":"main-responsibilities","dir":"Reference","previous_headings":"","what":"Main Responsibilities","title":"Scraper Configuration Class — cfg_scraper","text":"Define expose sensible default settings scraping projects Optionally load overrides YAML configuration file Allow programmatic overrides via ... Validate top-level nested configuration entries (e.g. robots, selenium) Provide convenient interface access update nested settings","code":""},{"path":"https://statistikat.github.io/taRantula/reference/paramsScraper.html","id":"top-level-configuration-structure","dir":"Reference","previous_headings":"","what":"Top-level Configuration Structure","title":"Scraper Configuration Class — cfg_scraper","text":"default configuration contains following top-level entries: project – Name scraping project (used organizing outputs/logs) base_dir – Base directory project-related data stored urls – Character vector URLs scraped robots – List robots.txt-related settings httr – List options httr::GET() calls selenium – List Selenium-related configuration See defaults() exact structure default values.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/paramsScraper.html","id":"super-class","dir":"Reference","previous_headings":"","what":"Super class","title":"Scraper Configuration Class — cfg_scraper","text":"taRantula::params_manager -> cfg_scraper","code":""},{"path":"https://statistikat.github.io/taRantula/reference/paramsScraper.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Scraper Configuration Class — cfg_scraper","text":"taRantula::params_manager$export() taRantula::params_manager$get() taRantula::params_manager$load_config() taRantula::params_manager$print() taRantula::params_manager$set() taRantula::params_manager$show_config() taRantula::params_manager$update() taRantula::params_manager$write_defaults()","code":""},{"path":"https://statistikat.github.io/taRantula/reference/paramsScraper.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Scraper Configuration Class — cfg_scraper","text":"cfg_scraper$new() cfg_scraper$defaults() cfg_scraper$clone()","code":""},{"path":"https://statistikat.github.io/taRantula/reference/paramsScraper.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Scraper Configuration Class — cfg_scraper","text":"Initialize new cfg_scraper configuration object. Configuration resolved following order: Built-defaults defined defaults() Optional YAML configuration file (config_file) Programmatic overrides passed via ... addition, Selenium configuration post-processed selenium$ecaps$args vector contains --user-agent= entry matching configured selenium$user_agent.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/paramsScraper.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Scraper Configuration Class — cfg_scraper","text":"","code":"cfg_scraper$new(config_file = NULL, base_dir = getwd(), ...)"},{"path":"https://statistikat.github.io/taRantula/reference/paramsScraper.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Scraper Configuration Class — cfg_scraper","text":"config_file Optional path YAML configuration file. base_dir Character string specifying base directory project-related data stored. Defaults getwd(). ... Named arguments override specific configuration settings. values take precedence defaults YAML file entries. Commonly used overrides include: project (character): Project name used file directory structures; default \"-project\". urls (character vector): URLs scraped; default character(0). robots (list): Settings related robots.txt handling: check (logical): Respect robots.txt? Default TRUE. snapshot_every (integer): Snapshot interval robots checks; default 10. workers (integer): Number parallel workers robots checks; default 1. robots_user_agent (character): User agent string robots queries; default .default_useragent(). httr (list): Configuration httr::GET()-based requests: user_agent (character): User agent string; default .default_useragent(). selenium (list): Selenium-related configuration: use_selenium (logical): Use Selenium? Default TRUE. host (character): Selenium server host; default \"localhost\". port (integer): Selenium server port; default 4444L. verbose (logical): Verbose Selenium output; default FALSE. browser (character): Browser name (e.g. \"chrome\"); default \"chrome\". user_agent (character): User agent Selenium; default .default_useragent(). ecaps (list): Extra capabilities: args (character vector): Chrome command-line arguments. prefs (list): Browser preferences (e.g. popup settings). excludeSwitches (character vector): Chrome switches exclude. snapshot_every (integer): Snapshot interval Selenium scraping; default 10L. workers (integer): Number parallel Selenium workers; default 1L.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/paramsScraper.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Scraper Configuration Class — cfg_scraper","text":"new cfg_scraper object.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/paramsScraper.html","id":"method-defaults-","dir":"Reference","previous_headings":"","what":"Method defaults()","title":"Scraper Configuration Class — cfg_scraper","text":"Return default configuration values scraper. defaults define complete, valid configuration robots handling Selenium-based scraping. Users can override values via YAML programmatic arguments.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/paramsScraper.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Scraper Configuration Class — cfg_scraper","text":"","code":"cfg_scraper$defaults()"},{"path":"https://statistikat.github.io/taRantula/reference/paramsScraper.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Scraper Configuration Class — cfg_scraper","text":"named list default configuration values.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/paramsScraper.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Scraper Configuration Class — cfg_scraper","text":"objects class cloneable method.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/paramsScraper.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Scraper Configuration Class — cfg_scraper","text":"","code":"cfg_scraper$clone(deep = FALSE)"},{"path":"https://statistikat.github.io/taRantula/reference/paramsScraper.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Scraper Configuration Class — cfg_scraper","text":"deep Whether make deep clone.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/paramsScraper.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Scraper Configuration Class — cfg_scraper","text":"","code":"# Create with defaults cfg <- paramsScraper() #> ℹ No configuration file provided for 'cfg_scraper'. Using default configuration.  # Create with overrides cfg <- paramsScraper(base_dir = tempdir(), project = \"my-project\") #> ℹ No configuration file provided for 'cfg_scraper'. Using default configuration.  # Write current configuration to file f <- tempfile(fileext = \".yaml\") cfg$export(f) #> ℹ Current configuration for 'cfg_scraper' written to '/tmp/RtmppVykHU/file1b746f3bec68.yaml'.  # Load from exported config-file and override cfg <- paramsScraper(config_file = f, project = \"some-other-proj\") #> ℹ Configuration loaded from '/tmp/RtmppVykHU/file1b746f3bec68.yaml' for cfg_scraper try(file.remove(f)) #> [1] TRUE  # Return the current configuration cfg$show_config() #> $project #> [1] \"some-other-proj\" #>  #> $base_dir #> [1] \"/tmp/RtmppVykHU/file1b7447f41c24/reference\" #>  #> $urls #> list() #>  #> $robots #> $robots$check #> [1] TRUE #>  #> $robots$snapshot_every #> [1] 10 #>  #> $robots$workers #> [1] 1 #>  #> $robots$robots_user_agent #> [1] \"Mozilla/5.0 (Macintosh; Intel Mac OS X 13_5_2) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15\" #>  #>  #> $httr #> $httr$user_agent #> [1] \"Mozilla/5.0 (Macintosh; Intel Mac OS X 13_5_2) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15\" #>  #>  #> $selenium #> $selenium$use_selenium #> [1] TRUE #>  #> $selenium$host #> [1] \"localhost\" #>  #> $selenium$port #> [1] 4444 #>  #> $selenium$verbose #> [1] FALSE #>  #> $selenium$browser #> [1] \"chrome\" #>  #> $selenium$user_agent #> [1] \"Mozilla/5.0 (Macintosh; Intel Mac OS X 13_5_2) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15\" #>  #> $selenium$ecaps #> $selenium$ecaps$args #>  [1] \"--headless\"                                                                                                                        #>  [2] \"--enable-automation\"                                                                                                               #>  [3] \"--disable-gpu\"                                                                                                                     #>  [4] \"--no-sandbox\"                                                                                                                      #>  [5] \"--start-maximized\"                                                                                                                 #>  [6] \"--disable-infobars\"                                                                                                                #>  [7] \"--disk-cache-size=400000000\"                                                                                                       #>  [8] \"--disable-browser-side-navigation\"                                                                                                 #>  [9] \"--disable-blink-features\"                                                                                                          #> [10] \"--window-size=1080,1920\"                                                                                                           #> [11] \"--disable-popup-blocking\"                                                                                                          #> [12] \"--disable-dev-shm-usage\"                                                                                                           #> [13] \"--lang=de\"                                                                                                                         #> [14] \"--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 13_5_2) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15\" #> [15] \"--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 13_5_2) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15\" #>  #> $selenium$ecaps$prefs #> $selenium$ecaps$prefs$PageLoadStrategy #> [1] \"eager\" #>  #> $selenium$ecaps$prefs$profile.default_content_settings.popups #> [1] 0 #>  #>  #> $selenium$ecaps$excludeSwitches #> [1] \"disable-popup-blocking\" #>  #>  #> $selenium$snapshot_every #> [1] 10 #>  #> $selenium$workers #> [1] 1 #>  #>   # Retrieve specific settings cfg$get(\"project\") #> [1] \"some-other-proj\" cfg$get(\"selenium$host\")          # nested via $-syntax #> [1] \"localhost\" cfg$get(c(\"selenium\", \"port\"))    # nested via character vector #> [1] 4444  # Update the configuration cfg$set(c(\"selenium\", \"port\"), 4445) cfg$set(\"selenium$host\", \"127.0.0.1\")"},{"path":"https://statistikat.github.io/taRantula/reference/params_manager.html","id":null,"dir":"Reference","previous_headings":"","what":"Parameter Manager Base Class — params_manager","title":"Parameter Manager Base Class — params_manager","text":"params_manager R6 base class managing hierarchical configuration parameters. supports: Built‑default settings defined subclasses Optional overrides via YAML configuration file Programmatic overrides via named arguments Nested key access using $ syntax character vectors class designed inherited specialized configuration classes (e.g., scrapers API clients) provides consistent, validated mechanism reading, updating, exporting configuration settings.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/params_manager.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Parameter Manager Base Class — params_manager","text":"R6::R6Class generator object.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/params_manager.html","id":"features","dir":"Reference","previous_headings":"","what":"Features","title":"Parameter Manager Base Class — params_manager","text":"YAML read/write support (via yaml package) Path syntax support: \"$b$c\" c(\"\",\"b\",\"c\") Nested configuration updating validation top-level key Export defaults current configuration YAML file","code":""},{"path":[]},{"path":"https://statistikat.github.io/taRantula/reference/params_manager.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Parameter Manager Base Class — params_manager","text":"params_manager$new() params_manager$print() params_manager$load_config() params_manager$get() params_manager$show_config() params_manager$defaults() params_manager$write_defaults() params_manager$export() params_manager$set() params_manager$update() params_manager$clone()","code":""},{"path":"https://statistikat.github.io/taRantula/reference/params_manager.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Parameter Manager Base Class — params_manager","text":"Initialize parameter manager. loads configuration using following precedence: Defaults defined subclass YAML configuration file (provided) Programmatic overrides via ...","code":""},{"path":"https://statistikat.github.io/taRantula/reference/params_manager.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parameter Manager Base Class — params_manager","text":"","code":"params_manager$new(config_file = NULL, ...)"},{"path":"https://statistikat.github.io/taRantula/reference/params_manager.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parameter Manager Base Class — params_manager","text":"config_file Optional path YAML configuration file. ... Named overrides applied defaults YAML values.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/params_manager.html","id":"method-print-","dir":"Reference","previous_headings":"","what":"Method print()","title":"Parameter Manager Base Class — params_manager","text":"Print current settings","code":""},{"path":"https://statistikat.github.io/taRantula/reference/params_manager.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Parameter Manager Base Class — params_manager","text":"","code":"params_manager$print()"},{"path":"https://statistikat.github.io/taRantula/reference/params_manager.html","id":"method-load-config-","dir":"Reference","previous_headings":"","what":"Method load_config()","title":"Parameter Manager Base Class — params_manager","text":"Loads configuration: defaults < YAML < args","code":""},{"path":"https://statistikat.github.io/taRantula/reference/params_manager.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Parameter Manager Base Class — params_manager","text":"","code":"params_manager$load_config(config_file = NULL, ...)"},{"path":"https://statistikat.github.io/taRantula/reference/params_manager.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parameter Manager Base Class — params_manager","text":"config_file (Optional) YAML file path. ... Named overrides apply last.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/params_manager.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Parameter Manager Base Class — params_manager","text":"list resulting configuration.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/params_manager.html","id":"method-get-","dir":"Reference","previous_headings":"","what":"Method get()","title":"Parameter Manager Base Class — params_manager","text":"Retrieve configuration value (supports nested paths). Nested syntax examples: \"selenium$host\" \"robots$check\" c(\"selenium\", \"port\")","code":""},{"path":"https://statistikat.github.io/taRantula/reference/params_manager.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Parameter Manager Base Class — params_manager","text":"","code":"params_manager$get(key)"},{"path":"https://statistikat.github.io/taRantula/reference/params_manager.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parameter Manager Base Class — params_manager","text":"key string using $ character vector nested keys.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/params_manager.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Parameter Manager Base Class — params_manager","text":"configuration value NULL found.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/params_manager.html","id":"method-show-config-","dir":"Reference","previous_headings":"","what":"Method show_config()","title":"Parameter Manager Base Class — params_manager","text":"Return complete current configuration list.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/params_manager.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Parameter Manager Base Class — params_manager","text":"","code":"params_manager$show_config()"},{"path":"https://statistikat.github.io/taRantula/reference/params_manager.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Parameter Manager Base Class — params_manager","text":"named list representing current configuration.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/params_manager.html","id":"method-defaults-","dir":"Reference","previous_headings":"","what":"Method defaults()","title":"Parameter Manager Base Class — params_manager","text":"Subclasses must override method define default settings.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/params_manager.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"Parameter Manager Base Class — params_manager","text":"","code":"params_manager$defaults()"},{"path":"https://statistikat.github.io/taRantula/reference/params_manager.html","id":"returns-3","dir":"Reference","previous_headings":"","what":"Returns","title":"Parameter Manager Base Class — params_manager","text":"named list containing default configuration values.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/params_manager.html","id":"method-write-defaults-","dir":"Reference","previous_headings":"","what":"Method write_defaults()","title":"Parameter Manager Base Class — params_manager","text":"Write default configuration values YAML file.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/params_manager.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"Parameter Manager Base Class — params_manager","text":"","code":"params_manager$write_defaults(filename)"},{"path":"https://statistikat.github.io/taRantula/reference/params_manager.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parameter Manager Base Class — params_manager","text":"filename Output file ending .yaml.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/params_manager.html","id":"method-export-","dir":"Reference","previous_headings":"","what":"Method export()","title":"Parameter Manager Base Class — params_manager","text":"Export current configuration (including overrides) YAML.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/params_manager.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"Parameter Manager Base Class — params_manager","text":"","code":"params_manager$export(filename)"},{"path":"https://statistikat.github.io/taRantula/reference/params_manager.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parameter Manager Base Class — params_manager","text":"filename Output file ending .yaml.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/params_manager.html","id":"method-set-","dir":"Reference","previous_headings":"","what":"Method set()","title":"Parameter Manager Base Class — params_manager","text":"Updates configuration. key top-level name*, specific element modified. key nested path* (e.g., \"x.y\" / \"x$y\" / c(\"x\",\"y\")), method updates nested value. cases, relevant top-level key validated via private$.validate().","code":""},{"path":"https://statistikat.github.io/taRantula/reference/params_manager.html","id":"usage-8","dir":"Reference","previous_headings":"","what":"Usage","title":"Parameter Manager Base Class — params_manager","text":"","code":"params_manager$set(key, val)"},{"path":"https://statistikat.github.io/taRantula/reference/params_manager.html","id":"arguments-5","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parameter Manager Base Class — params_manager","text":"key top-level key nested path (\"$b$c\" c(\"\",\"b\",\"c\")) val New value assign.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/params_manager.html","id":"returns-4","dir":"Reference","previous_headings":"","what":"Returns","title":"Parameter Manager Base Class — params_manager","text":"object (invisibly).","code":""},{"path":"https://statistikat.github.io/taRantula/reference/params_manager.html","id":"method-update-","dir":"Reference","previous_headings":"","what":"Method update()","title":"Parameter Manager Base Class — params_manager","text":"Recursively update configuration values (possibly nested) list. top-level keys contained values validated.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/params_manager.html","id":"usage-9","dir":"Reference","previous_headings":"","what":"Usage","title":"Parameter Manager Base Class — params_manager","text":"","code":"params_manager$update(values)"},{"path":"https://statistikat.github.io/taRantula/reference/params_manager.html","id":"arguments-6","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parameter Manager Base Class — params_manager","text":"values named list merged current configuration.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/params_manager.html","id":"returns-5","dir":"Reference","previous_headings":"","what":"Returns","title":"Parameter Manager Base Class — params_manager","text":"object (invisibly).","code":""},{"path":"https://statistikat.github.io/taRantula/reference/params_manager.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Parameter Manager Base Class — params_manager","text":"objects class cloneable method.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/params_manager.html","id":"usage-10","dir":"Reference","previous_headings":"","what":"Usage","title":"Parameter Manager Base Class — params_manager","text":"","code":"params_manager$clone(deep = FALSE)"},{"path":"https://statistikat.github.io/taRantula/reference/params_manager.html","id":"arguments-7","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parameter Manager Base Class — params_manager","text":"deep Whether make deep clone.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/parse_HTML.html","id":null,"dir":"Reference","previous_headings":"","what":"Parse HTML and Remove Non‑Text Elements — parse_HTML","title":"Parse HTML and Remove Non‑Text Elements — parse_HTML","text":"Converts HTML document cleaned representation scripts, styles, similar elements removed. keep_only_text = TRUE, function returns visible text page.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/parse_HTML.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parse HTML and Remove Non‑Text Elements — parse_HTML","text":"","code":"parse_HTML(doc, keep_only_text = FALSE)"},{"path":"https://statistikat.github.io/taRantula/reference/parse_HTML.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parse HTML and Remove Non‑Text Elements — parse_HTML","text":"doc Either HTML content character string xml_document. NA inputs returned unchanged. keep_only_text Logical; TRUE, returns human‑readable text.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/parse_HTML.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Parse HTML and Remove Non‑Text Elements — parse_HTML","text":"cleaned XML node set character string (keep_only_text = TRUE).","code":""},{"path":"https://statistikat.github.io/taRantula/reference/parse_HTML.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Parse HTML and Remove Non‑Text Elements — parse_HTML","text":"helper used prepare HTML content downstream text extraction. : Removes <script>, <style>, <noscript> nodes Optionally extracts visible text Supports raw HTML input already parsed XML documents","code":""},{"path":"https://statistikat.github.io/taRantula/reference/query_robotsdata.html","id":null,"dir":"Reference","previous_headings":"","what":"Query Stored robots.txt Permissions for a Given URL — query_robotsdata","title":"Query Stored robots.txt Permissions for a Given URL — query_robotsdata","text":"Retrieves stored robots.txt permissions domain given URL DuckDB database returns robotstxt object.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/query_robotsdata.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Query Stored robots.txt Permissions for a Given URL — query_robotsdata","text":"","code":"query_robotsdata(db_file, url)"},{"path":"https://statistikat.github.io/taRantula/reference/query_robotsdata.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Query Stored robots.txt Permissions for a Given URL — query_robotsdata","text":"db_file character(1) Path DuckDB database file. url character(1) URL stored robots.txt information retrieved.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/query_robotsdata.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Query Stored robots.txt Permissions for a Given URL — query_robotsdata","text":"robotstxt object robotstxt package.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/query_robotsdata.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Query Stored robots.txt Permissions for a Given URL — query_robotsdata","text":"domain exist \"robots\" table, function returns robotstxt object empty permissions, implying full access.","code":""},{"path":[]},{"path":"https://statistikat.github.io/taRantula/reference/query_robotsdata.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Query Stored robots.txt Permissions for a Given URL — query_robotsdata","text":"","code":"if (FALSE) { # \\dontrun{ query_robotsdata(\"robots.duckdb\", \"https://example.com/page\") } # }"},{"path":"https://statistikat.github.io/taRantula/reference/read_json_wrapper.html","id":null,"dir":"Reference","previous_headings":"","what":"Helper: JSON Reader with Retry Logic — read_json_wrapper","title":"Helper: JSON Reader with Retry Logic — read_json_wrapper","text":"Internal helper reads JSON URL, automatically retrying exponential backoff errors warnings occur.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/read_json_wrapper.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Helper: JSON Reader with Retry Logic — read_json_wrapper","text":"","code":"read_json_wrapper(path, count = 1)"},{"path":"https://statistikat.github.io/taRantula/reference/read_json_wrapper.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Helper: JSON Reader with Retry Logic — read_json_wrapper","text":"path URL JSON read. count Integer specifying current retry interval. Used internally.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/read_json_wrapper.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Helper: JSON Reader with Retry Logic — read_json_wrapper","text":"Parsed JSON content error/warning object retries fail.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/runGoogleSearch.html","id":null,"dir":"Reference","previous_headings":"","what":"Run Google Search Workflow — runGoogleSearch","title":"Run Google Search Workflow — runGoogleSearch","text":"Executes one multiple Google Custom Search API queries derived prepared dataset. Results saved directory structure defined provided cfg_googlesearch() configuration object.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/runGoogleSearch.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run Google Search Workflow — runGoogleSearch","text":"","code":"runGoogleSearch(cfg = cfg_googlesearch$new(), dat)"},{"path":"https://statistikat.github.io/taRantula/reference/runGoogleSearch.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run Google Search Workflow — runGoogleSearch","text":"cfg cfg_googlesearch() configuration object containing required search, credential, file‑handling settings. dat data.table containing variables referenced cfg$query_col. referenced columns must exist dat.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/runGoogleSearch.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Run Google Search Workflow — runGoogleSearch","text":"Returns TRUE invisibly queries completed successfully. Result files written directory specified cfg.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/runGoogleSearch.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Run Google Search Workflow — runGoogleSearch","text":"","code":"## Example use will be added in future releases"},{"path":"https://statistikat.github.io/taRantula/reference/searchURL.html","id":null,"dir":"Reference","previous_headings":"","what":"Search Google for Business URLs — searchURL","title":"Search Google for Business URLs — searchURL","text":"Sends search queries Google Custom Search API identify URLs associated businesses. function processes queries provided data frame returns link results together positional information.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/searchURL.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Search Google for Business URLs — searchURL","text":"","code":"searchURL(cfg, dat, file = file, query_col = query_col)"},{"path":"https://statistikat.github.io/taRantula/reference/searchURL.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Search Google for Business URLs — searchURL","text":"cfg cfg_googlesearch() configuration object containing API credentials query-related settings. dat data.frame data.table containing variables search queries constructed. file Optional CSV filename results written. file exists overwrite = FALSE set cfg, previously saved results loaded skipped. query_col Character scalar; name column dat contains encoded search queries created via buildQuery().","code":""},{"path":"https://statistikat.github.io/taRantula/reference/searchURL.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Search Google for Business URLs — searchURL","text":"data.table containing following columns: idcol – Identifier business attributes – Fields extracted Google results (e.g., link, title) position – Search result ranking position","code":""},{"path":"https://statistikat.github.io/taRantula/reference/searchURL.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Search Google for Business URLs — searchURL","text":"use function, must provide valid Google Custom Search API key via environment variable SCRAPING_APIKEY_GOOGLE, example adding ~/.Renviron file. addition, Custom Search Engine (CSE) identifier must available via environment variable SCRAPING_ENGINE_GOOGLE.","code":""},{"path":"https://statistikat.github.io/taRantula/reference/searchURL.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Search Google for Business URLs — searchURL","text":"","code":"## Example usage will be added in future versions"},{"path":[]},{"path":"https://statistikat.github.io/taRantula/news/index.html","id":"main-features-0-1-0","dir":"Changelog","previous_headings":"","what":"Main Features","title":"taRantula 0.1.0","text":"Persistent Storage: Implemented DuckDB backend scraping jobs. ensures data persisted disk immediately, preventing data loss allowing standard SQL querying results. Selenium Grid Integration: Full support Selenium 4 Hub/Node architectures. system optimized containerized environments high memory demands. Redirect Detection: Introduced logic detect log URL redirects comparing initial request URLs final browser state; results stored url_redirect field. Fault Tolerance: Introduced snapshotting mechanism periodically saves worker progress. allows scraper resume last stable state event system network crash. Parallel Processing: Integrated future future.apply multi-worker scraping, enabling simultaneous browser sessions across Selenium Grid.","code":""},{"path":"https://statistikat.github.io/taRantula/news/index.html","id":"configuration-params_manager-0-1-0","dir":"Changelog","previous_headings":"","what":"Configuration (params_manager)","title":"taRantula 0.1.0","text":"paramsScraper(): Dedicated configuration generic web crawling JS rendering. paramsGoogleSearch(): Tailored configuration Google Search API interactions including rate-limit management. Deep Merging: Configuration methods now support nested path updates (e.g., cfg$set(\"selenium$host\", ...)). Validation: Built-defensive programming type-checking integers, booleans, character vectors, directory paths. Export/Import functionality: Added $export() $write_defaults() methods support YAML-based configuration round-trips.","code":""},{"path":"https://statistikat.github.io/taRantula/news/index.html","id":"scraping-implementation-urlscraper-0-1-0","dir":"Changelog","previous_headings":"","what":"Scraping Implementation (UrlScraper)","title":"taRantula 0.1.0","text":"Hybrid Engine Support: Implemented polymorphic scraping logic switches Selenium httr (high-speed static scraping) based configuration. Regex Extraction: Added $regex_extract() method high-performance data mining (e.g., extracting VAT/UID numbers Email addresses) directly persistent database. Compliance: Automated robots.txt enforcement internal cache reduce overhead hitting domain multiple times. Graceful Termination: Implemented $stop() signaling mechanism allows parallel workers finish current URL exit cleanly without corrupting DuckDB file.","code":""},{"path":"https://statistikat.github.io/taRantula/news/index.html","id":"documentation--testing-0-1-0","dir":"Changelog","previous_headings":"","what":"Documentation & Testing","title":"taRantula 0.1.0","text":"Vignettes: Created “Getting Started” guide covering dual-engine setup production-ready docker-compose templates. Unit Tests: Implemented testthat suite containing unit-tests, mainly configuration part.","code":""}]
