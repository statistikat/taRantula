<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper • taRantula</title><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet"><link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet"><script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="UrlScraper R6 Class for Parallel Web Scraping with Selenium — UrlScraper"><meta name="description" content="The UrlScraper R6 class provides a high‑level framework for scraping
a list of URLs using multiple parallel Selenium (or non‑Selenium) workers.
It manages scraping state, progress, snapshots, logs, and respects
robots.txt rules. Results and logs are stored in an internal DuckDB
database."><meta property="og:description" content="The UrlScraper R6 class provides a high‑level framework for scraping
a list of URLs using multiple parallel Selenium (or non‑Selenium) workers.
It manages scraping state, progress, snapshots, logs, and respects
robots.txt rules. Results and logs are stored in an internal DuckDB
database."></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-primary" data-bs-theme="dark" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">taRantula</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.1.0</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="nav-item"><a class="nav-link" href="../articles/Intro.html">Get Started</a></li>
<li class="active nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item"><a class="nav-link" href="../articles/index.html">Articles</a></li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul><ul class="navbar-nav"><li class="nav-item"><a class="external-link nav-link" href="https://github.com/statistikat/taRantula/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul></div>


  </div>
</nav><div class="container template-reference-topic">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>UrlScraper R6 Class for Parallel Web Scraping with Selenium</h1>
      <small class="dont-index">Source: <a href="https://github.com/statistikat/taRantula/blob/master/R/UrlScraper.R" class="external-link"><code>R/UrlScraper.R</code></a></small>
      <div class="d-none name"><code>UrlScraper.Rd</code></div>
    </div>

    <div class="ref-description section level2">
    <p>The <code>UrlScraper</code> R6 class provides a high‑level framework for scraping
a list of URLs using multiple parallel Selenium (or non‑Selenium) workers.
It manages scraping state, progress, snapshots, logs, and respects
<code>robots.txt</code> rules. Results and logs are stored in an internal DuckDB
database.</p>
    </div>


    <div class="section level2">
    <h2 id="format">Format<a class="anchor" aria-label="anchor" href="#format"></a></h2>
    <p>An R6 class generator of class <code>UrlScraper</code>.</p>
    </div>
    <div class="section level2">
    <h2 id="overview">Overview<a class="anchor" aria-label="anchor" href="#overview"></a></h2>


<p>The <code>UrlScraper</code> class is designed for robust, resumable web scraping
workflows. Its key features include:</p><ul><li><p>Parallel scraping of URLs via multiple Selenium workers</p></li>
<li><p>Persistent storage of results, logs, and extracted links in DuckDB</p></li>
<li><p>Automatic snapshotting and recovery of partially processed chunks</p></li>
<li><p>Respecting <code>robots.txt</code> rules via pre‑checks on domains</p></li>
<li><p>Convenience helpers for querying results, logs, and extracted links</p></li>
<li><p>Regex‑based extraction of text from previously scraped HTML</p></li>
</ul></div>
    <div class="section level2">
    <h2 id="configuration">Configuration<a class="anchor" aria-label="anchor" href="#configuration"></a></h2>


<p>A configuration object (typically created via <a href="paramsScraper.html">paramsScraper</a>) is
expected to contain at least the following entries:</p><ul><li><p><code>db_file</code> – path to the DuckDB database file</p></li>
<li><p><code>snapshot_dir</code> – directory for temporary snapshot files</p></li>
<li><p><code>progress_dir</code> – directory for progress/log files</p></li>
<li><p><code>stop_file</code> – path to a file used to signal workers to stop</p></li>
<li><p><code>urls</code> / <code>urls_todo</code> – vectors of URLs and URLs still to scrape</p></li>
<li><p><code>selenium</code> – list with Selenium‑related settings, such as:</p><ul><li><p><code>use_selenium</code> – logical, whether to use Selenium</p></li>
<li><p><code>workers</code> – number of parallel Selenium workers</p></li>
<li><p><code>host</code>, <code>port</code>, <code>browser</code>, <code>verbose</code> – Selenium connection settings</p></li>
<li><p><code>ecaps</code> – list with Chrome options (<code>args</code>, <code>prefs</code>, <code>excludeSwitches</code>)</p></li>
<li><p><code>snapshot_every</code> – number of URLs after which a snapshot is taken</p></li>
</ul></li>
<li><p><code>robots</code> – list with <code>robots.txt</code> handling options, such as:</p><ul><li><p><code>check</code> – logical, whether to check <code>robots.txt</code></p></li>
<li><p><code>snapshot_every</code> – snapshot frequency for robots checks</p></li>
<li><p><code>workers</code> – number of workers for <code>robots.txt</code> checks</p></li>
<li><p><code>robots_user_agent</code> – user agent string used for robots queries</p></li>
</ul></li>
<li><p><code>exclude_social_links</code> – logical, whether to exclude social media links</p></li>
</ul><p>The exact structure depends on <a href="paramsScraper.html">paramsScraper</a> and related helpers.</p>
    </div>
    <div class="section level2">
    <h2 id="methods">Methods<a class="anchor" aria-label="anchor" href="#methods"></a></h2>


<ul><li><p><code>initialize(config)</code> – create a new <code>UrlScraper</code> instance</p></li>
<li><p><code>scrape()</code> – scrape all remaining URLs in parallel</p></li>
<li><p><code>update_urls(urls, force = FALSE)</code> – add new URLs to the queue</p></li>
<li><p><code>results(filter = NULL)</code> – extract scraping results</p></li>
<li><p><code>logs(filter = NULL)</code> – extract log entries</p></li>
<li><p><code>links(filter = NULL)</code> – extract discovered links</p></li>
<li><p><code>query(q)</code> – run custom SQL queries on the internal DuckDB database</p></li>
<li><p><code>regex_extract(pattern, group = NULL, filter_links = NULL, ignore_cases = TRUE)</code> – extract text via regex from scraped HTML</p></li>
<li><p><code><a href="https://rdrr.io/r/base/stop.html" class="external-link">stop()</a></code> – create a stop‑file so workers can exit gracefully</p></li>
<li><p><code><a href="https://rdrr.io/r/base/connections.html" class="external-link">close()</a></code> – clean up snapshots and close database connections</p></li>
</ul></div>
    <div class="section level2">
    <h2 id="methods-1">Methods<a class="anchor" aria-label="anchor" href="#methods-1"></a></h2>

<div class="section">
<h3 id="public-methods">Public methods<a class="anchor" aria-label="anchor" href="#public-methods"></a></h3>

<ul><li><p><a href="#method-UrlScraper-new"><code>UrlScraper$new()</code></a></p></li>
<li><p><a href="#method-UrlScraper-scrape"><code>UrlScraper$scrape()</code></a></p></li>
<li><p><a href="#method-UrlScraper-update_urls"><code>UrlScraper$update_urls()</code></a></p></li>
<li><p><a href="#method-UrlScraper-results"><code>UrlScraper$results()</code></a></p></li>
<li><p><a href="#method-UrlScraper-logs"><code>UrlScraper$logs()</code></a></p></li>
<li><p><a href="#method-UrlScraper-links"><code>UrlScraper$links()</code></a></p></li>
<li><p><a href="#method-UrlScraper-query"><code>UrlScraper$query()</code></a></p></li>
<li><p><a href="#method-UrlScraper-regex_extract"><code>UrlScraper$regex_extract()</code></a></p></li>
<li><p><a href="#method-UrlScraper-stop"><code>UrlScraper$stop()</code></a></p></li>
<li><p><a href="#method-UrlScraper-close"><code>UrlScraper$close()</code></a></p></li>
<li><p><a href="#method-UrlScraper-clone"><code>UrlScraper$clone()</code></a></p></li>
</ul></div><p></p><hr><a id="method-UrlScraper-new"></a><div class="section">
<h3 id="method-new-">Method <code>new()</code><a class="anchor" aria-label="anchor" href="#method-new-"></a></h3>
<p>Create a new <code>UrlScraper</code> object.</p>
<p>This constructor initializes the internal storage (DuckDB database,
snapshot and progress directories), restores previous snapshots/logs
if present, and configures progress handlers.</p><div class="section">
<h4 id="usage">Usage<a class="anchor" aria-label="anchor" href="#usage"></a></h4>
<p></p><div class="r"><div class="sourceCode"><pre><code><span><span class="va"><a href="../reference/UrlScraper.html">UrlScraper</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="va">config</span><span class="op">)</span></span></code></pre></div><p></p></div>
</div>

<div class="section">
<h4 id="arguments">Arguments<a class="anchor" aria-label="anchor" href="#arguments"></a></h4>
<p></p><div class="arguments"><dl><dt><code>config</code></dt>
<dd><p>A list (or configuration object) of settings, typically
created by <code><a href="paramsScraper.html">paramsScraper()</a></code>. It should include:</p><ul><li><p><code>db_file</code> – path to the DuckDB database file.</p></li>
<li><p><code>snapshot_dir</code> – directory for snapshot files.</p></li>
<li><p><code>progress_dir</code> – directory for progress/log files.</p></li>
<li><p><code>stop_file</code> – path to the stop signal file.</p></li>
<li><p><code>urls_todo</code> – character vector of URLs still to be scraped.</p></li>
<li><p><code>selenium</code> – list of Selenium settings (host, port, workers, etc.).</p></li>
<li><p><code>robots</code> – list of <code>robots.txt</code> handling options.</p></li>
<li><p>any additional options required by helper functions.</p></li>
</ul></dd>


</dl><p></p></div>
</div>
<div class="section">
<h4 id="returns">Returns<a class="anchor" aria-label="anchor" href="#returns"></a></h4>
<p>An initialized <code>UrlScraper</code> object (invisibly).</p>
</div>

</div><p></p><hr><a id="method-UrlScraper-scrape"></a><div class="section">
<h3 id="method-scrape-">Method <code>scrape()</code><a class="anchor" aria-label="anchor" href="#method-scrape-"></a></h3>
<p>Scrape all remaining URLs using parallel workers.</p><div class="section">
<h4 id="usage-1">Usage<a class="anchor" aria-label="anchor" href="#usage-1"></a></h4>
<p></p><div class="r"><div class="sourceCode"><pre><code><span><span class="va">UrlScraper</span><span class="op">$</span><span class="fu">scrape</span><span class="op">(</span><span class="op">)</span></span></code></pre></div><p></p></div>
</div>

<div class="section">
<h4 id="details">Details<a class="anchor" aria-label="anchor" href="#details"></a></h4>
<p>This method orchestrates the parallel scraping process:</p><ul><li><p>Re‑initializes storage and processes any existing snapshots or logs.</p></li>
<li><p>Computes the set of URLs still to scrape.</p></li>
<li><p>Optionally performs <code>robots.txt</code> checks on new domains.</p></li>
<li><p>Sets up a parallel plan via the <code>future</code> framework.</p></li>
<li><p>Starts multiple Selenium (or non‑Selenium) sessions.</p></li>
<li><p>Distributes URLs across workers and tracks global progress.</p></li>
<li><p>Cleans up snapshots/logs and updates internal URL state after scraping.</p></li>
</ul><p>If a stop‑file is detected (see <code><a href="https://rdrr.io/r/base/stop.html" class="external-link">stop()</a></code>), scraping is aborted
before starting. Workers themselves will also honor the stop‑file to
terminate gracefully after finishing the current URL.</p>
</div>

<div class="section">
<h4 id="returns-1">Returns<a class="anchor" aria-label="anchor" href="#returns-1"></a></h4>
<p>The <code>UrlScraper</code> object (invisibly), with internal state
updated to reflect newly scraped URLs.</p>
</div>

</div><p></p><hr><a id="method-UrlScraper-update_urls"></a><div class="section">
<h3 id="method-update-urls-">Method <code>update_urls()</code><a class="anchor" aria-label="anchor" href="#method-update-urls-"></a></h3>
<p>Update the list of URLs to be scraped.</p><div class="section">
<h4 id="usage-2">Usage<a class="anchor" aria-label="anchor" href="#usage-2"></a></h4>
<p></p><div class="r"><div class="sourceCode"><pre><code><span><span class="va">UrlScraper</span><span class="op">$</span><span class="fu">update_urls</span><span class="op">(</span><span class="va">urls</span>, force <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span></code></pre></div><p></p></div>
</div>

<div class="section">
<h4 id="arguments-1">Arguments<a class="anchor" aria-label="anchor" href="#arguments-1"></a></h4>
<p></p><div class="arguments"><dl><dt><code>urls</code></dt>
<dd><p>A character vector of new URLs to add.</p></dd>


<dt><code>force</code></dt>
<dd><p>A logical flag. If <code>TRUE</code>, all given URLs are kept except
for duplicates within <code>urls</code> itself (no check against already
scraped URLs). If <code>FALSE</code> (default), URLs already in the database
and duplicates in <code>urls</code> are removed.</p></dd>


</dl><p></p></div>
</div>
<div class="section">
<h4 id="details-1">Details<a class="anchor" aria-label="anchor" href="#details-1"></a></h4>
<p>This method updates the internal URL queue based on the given input
vector <code>urls</code>. Depending on <code>force</code>:</p><ul><li><p>If <code>force = FALSE</code> (default), URLs that have already been scraped
(i.e. present in the results database) are removed, as well as
duplicates within the <code>urls</code> vector itself.</p></li>
<li><p>If <code>force = TRUE</code>, only duplicates within the given <code>urls</code> vector
are removed; URLs that are already present in the database are kept.</p></li>
</ul><p>Summary information about how many URLs were added, already known, or
duplicates is printed via <code>cli</code>.</p>
</div>

<div class="section">
<h4 id="returns-2">Returns<a class="anchor" aria-label="anchor" href="#returns-2"></a></h4>
<p>The <code>UrlScraper</code> object (invisibly).</p>
</div>

</div><p></p><hr><a id="method-UrlScraper-results"></a><div class="section">
<h3 id="method-results-">Method <code>results()</code><a class="anchor" aria-label="anchor" href="#method-results-"></a></h3>
<p>Extract scraping results from the internal database.</p><div class="section">
<h4 id="usage-3">Usage<a class="anchor" aria-label="anchor" href="#usage-3"></a></h4>
<p></p><div class="r"><div class="sourceCode"><pre><code><span><span class="va">UrlScraper</span><span class="op">$</span><span class="fu">results</span><span class="op">(</span>filter <span class="op">=</span> <span class="cn">NULL</span><span class="op">)</span></span></code></pre></div><p></p></div>
</div>

<div class="section">
<h4 id="arguments-2">Arguments<a class="anchor" aria-label="anchor" href="#arguments-2"></a></h4>
<p></p><div class="arguments"><dl><dt><code>filter</code></dt>
<dd><p>Optional character string with a SQL‑like <code>WHERE</code>
condition (without the <code>WHERE</code> keyword), e.g.
<code>"url LIKE 'https://example.com/%'"</code>. If <code>NULL</code> (default), all rows
from the <code>results</code> table are returned.</p></dd>


</dl><p></p></div>
</div>
<div class="section">
<h4 id="returns-3">Returns<a class="anchor" aria-label="anchor" href="#returns-3"></a></h4>
<p>A <code>data.table</code> containing the scraping results.</p>
</div>

</div><p></p><hr><a id="method-UrlScraper-logs"></a><div class="section">
<h3 id="method-logs-">Method <code>logs()</code><a class="anchor" aria-label="anchor" href="#method-logs-"></a></h3>
<p>Extract log entries from the internal database.</p><div class="section">
<h4 id="usage-4">Usage<a class="anchor" aria-label="anchor" href="#usage-4"></a></h4>
<p></p><div class="r"><div class="sourceCode"><pre><code><span><span class="va">UrlScraper</span><span class="op">$</span><span class="fu">logs</span><span class="op">(</span>filter <span class="op">=</span> <span class="cn">NULL</span><span class="op">)</span></span></code></pre></div><p></p></div>
</div>

<div class="section">
<h4 id="arguments-3">Arguments<a class="anchor" aria-label="anchor" href="#arguments-3"></a></h4>
<p></p><div class="arguments"><dl><dt><code>filter</code></dt>
<dd><p>Optional character string with a SQL‑like <code>WHERE</code>
condition (without the <code>WHERE</code> keyword). If <code>NULL</code> (default), all
rows from the <code>logs</code> table are returned.</p></dd>


</dl><p></p></div>
</div>
<div class="section">
<h4 id="returns-4">Returns<a class="anchor" aria-label="anchor" href="#returns-4"></a></h4>
<p>A <code>data.table</code> containing the log entries.</p>
</div>

</div><p></p><hr><a id="method-UrlScraper-links"></a><div class="section">
<h3 id="method-links-">Method <code>links()</code><a class="anchor" aria-label="anchor" href="#method-links-"></a></h3>
<p>Extract scraped links from the internal database.</p><div class="section">
<h4 id="usage-5">Usage<a class="anchor" aria-label="anchor" href="#usage-5"></a></h4>
<p></p><div class="r"><div class="sourceCode"><pre><code><span><span class="va">UrlScraper</span><span class="op">$</span><span class="fu">links</span><span class="op">(</span>filter <span class="op">=</span> <span class="cn">NULL</span><span class="op">)</span></span></code></pre></div><p></p></div>
</div>

<div class="section">
<h4 id="arguments-4">Arguments<a class="anchor" aria-label="anchor" href="#arguments-4"></a></h4>
<p></p><div class="arguments"><dl><dt><code>filter</code></dt>
<dd><p>Optional character string with a SQL‑like <code>WHERE</code>
condition (without the <code>WHERE</code> keyword). If <code>NULL</code> (default), all
rows from the <code>links</code> table are returned.</p></dd>


</dl><p></p></div>
</div>
<div class="section">
<h4 id="returns-5">Returns<a class="anchor" aria-label="anchor" href="#returns-5"></a></h4>
<p>A <code>data.table</code> containing the extracted links.</p>
</div>

</div><p></p><hr><a id="method-UrlScraper-query"></a><div class="section">
<h3 id="method-query-">Method <code>query()</code><a class="anchor" aria-label="anchor" href="#method-query-"></a></h3>
<p>Execute a custom SQL query against the internal DuckDB database.</p>
<p>This is a low‑level helper for advanced use cases. It assumes that
the user is familiar with the schema of the internal database
(tables such as <code>results</code>, <code>logs</code>, <code>links</code>, and any others created
by helper functions).</p><div class="section">
<h4 id="usage-6">Usage<a class="anchor" aria-label="anchor" href="#usage-6"></a></h4>
<p></p><div class="r"><div class="sourceCode"><pre><code><span><span class="va">UrlScraper</span><span class="op">$</span><span class="fu">query</span><span class="op">(</span><span class="va">q</span><span class="op">)</span></span></code></pre></div><p></p></div>
</div>

<div class="section">
<h4 id="arguments-5">Arguments<a class="anchor" aria-label="anchor" href="#arguments-5"></a></h4>
<p></p><div class="arguments"><dl><dt><code>q</code></dt>
<dd><p>A character string containing a valid DuckDB SQL query.</p></dd>


</dl><p></p></div>
</div>
<div class="section">
<h4 id="returns-6">Returns<a class="anchor" aria-label="anchor" href="#returns-6"></a></h4>
<p>The result of the query, typically a <code>data.table</code>.</p>
</div>

</div><p></p><hr><a id="method-UrlScraper-regex_extract"></a><div class="section">
<h3 id="method-regex-extract-">Method <code>regex_extract()</code><a class="anchor" aria-label="anchor" href="#method-regex-extract-"></a></h3>
<p>Extract text from scraped HTML using a regular expression.</p><div class="section">
<h4 id="usage-7">Usage<a class="anchor" aria-label="anchor" href="#usage-7"></a></h4>
<p></p><div class="r"><div class="sourceCode"><pre><code><span><span class="va">UrlScraper</span><span class="op">$</span><span class="fu">regex_extract</span><span class="op">(</span></span>
<span>  <span class="va">pattern</span>,</span>
<span>  group <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  filter_links <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  ignore_cases <span class="op">=</span> <span class="cn">TRUE</span></span>
<span><span class="op">)</span></span></code></pre></div><p></p></div>
</div>

<div class="section">
<h4 id="arguments-6">Arguments<a class="anchor" aria-label="anchor" href="#arguments-6"></a></h4>
<p></p><div class="arguments"><dl><dt><code>pattern</code></dt>
<dd><p>A character string containing a regular expression.
Named capture groups are supported.</p></dd>


<dt><code>group</code></dt>
<dd><p>Either:</p><ul><li><p>A character string naming a capture group (e.g.
<code>"name"</code> if the pattern contains <code>(?&lt;name&gt;...)</code>), or</p></li>
<li><p>An integer specifying the index of the capture group to return.
If <code>NULL</code> (default), the behavior is delegated to <code><a href="dot-extract_regex.html">.extract_regex()</a></code>
and may return all groups depending on its implementation.</p></li>
</ul></dd>


<dt><code>filter_links</code></dt>
<dd><p>A character vector containing keywords or partial
words used to filter the set of URLs from which <code>pattern</code> will be
extracted. For example, <code>filter_links = "imprint"</code> restricts the
extraction to URLs whose <code>href</code> or <code>label</code> contains "imprint".</p></dd>


<dt><code>ignore_cases</code></dt>
<dd><p>Logical. If <code>TRUE</code> (default), case is ignored
when matching <code>pattern</code>. If <code>FALSE</code>, the pattern is matched in a
case‑sensitive way.</p></dd>


</dl><p></p></div>
</div>
<div class="section">
<h4 id="details-2">Details<a class="anchor" aria-label="anchor" href="#details-2"></a></h4>
<p>This helper performs a post‑processing step on the stored HTML
sources in the <code>results</code> table:</p><ol><li><p>It first selects links from the <code>links</code> table whose <code>href</code> or
<code>label</code> match the provided <code>filter_links</code> terms.</p></li>
<li><p>It then identifies those documents (rows in <code>results</code>) whose
<code>url</code> is among the selected links and that have <code>status == TRUE</code>.</p></li>
<li><p>Finally, it applies a regular expression to the HTML source of
those documents and returns the extracted matches.</p></li>
</ol><p>This is particularly useful for extracting structured information
such as email addresses, phone numbers, or IDs from a subset of
pages (e.g. contact or imprint pages).</p>
</div>

<div class="section">
<h4 id="returns-7">Returns<a class="anchor" aria-label="anchor" href="#returns-7"></a></h4>
<p>A <code>data.table</code> (or similar object) returned by
<code><a href="dot-extract_regex.html">.extract_regex()</a></code>, typically containing the matched text and the
corresponding URLs.</p>
</div>

</div><p></p><hr><a id="method-UrlScraper-stop"></a><div class="section">
<h3 id="method-stop-">Method <code><a href="https://rdrr.io/r/base/stop.html" class="external-link">stop()</a></code><a class="anchor" aria-label="anchor" href="#method-stop-"></a></h3>
<p>Create a stop‑file to signal running workers to terminate gracefully.</p><div class="section">
<h4 id="usage-8">Usage<a class="anchor" aria-label="anchor" href="#usage-8"></a></h4>
<p></p><div class="r"><div class="sourceCode"><pre><code><span><span class="va">UrlScraper</span><span class="op">$</span><span class="kw">stop</span><span class="op">(</span><span class="op">)</span></span></code></pre></div><p></p></div>
</div>

<div class="section">
<h4 id="details-3">Details<a class="anchor" aria-label="anchor" href="#details-3"></a></h4>
<p>Workers periodically check for the existence of the configured
<code>stop_file</code>. When it is present, they will finish processing the
current URL and then exit. This allows for a controlled shutdown
of a long‑running scraping job without abruptly terminating the
R session or Selenium instances.</p>
</div>

<div class="section">
<h4 id="returns-8">Returns<a class="anchor" aria-label="anchor" href="#returns-8"></a></h4>
<p>Invisible <code>NULL</code>.</p>
</div>

</div><p></p><hr><a id="method-UrlScraper-close"></a><div class="section">
<h3 id="method-close-">Method <code><a href="https://rdrr.io/r/base/connections.html" class="external-link">close()</a></code><a class="anchor" aria-label="anchor" href="#method-close-"></a></h3>
<p>Clean up resources, including snapshots and database connections.</p><div class="section">
<h4 id="usage-9">Usage<a class="anchor" aria-label="anchor" href="#usage-9"></a></h4>
<p></p><div class="r"><div class="sourceCode"><pre><code><span><span class="va">UrlScraper</span><span class="op">$</span><span class="fu">close</span><span class="op">(</span><span class="op">)</span></span></code></pre></div><p></p></div>
</div>

<div class="section">
<h4 id="details-4">Details<a class="anchor" aria-label="anchor" href="#details-4"></a></h4>
<p>This method performs the following clean‑up steps:</p><ul><li><p>Processes any remaining snapshots and logs.</p></li>
<li><p>Deletes the snapshot directory (if it exists).</p></li>
<li><p>Opens a DuckDB connection to the configured <code>db_file</code> and
disconnects it with <code>shutdown = TRUE</code>.</p></li>
</ul><p>It is good practice to call <code><a href="https://rdrr.io/r/base/connections.html" class="external-link">close()</a></code> once you are done with a
<code>UrlScraper</code> instance.</p>
</div>

<div class="section">
<h4 id="returns-9">Returns<a class="anchor" aria-label="anchor" href="#returns-9"></a></h4>
<p>Invisible <code>NULL</code>.</p>
</div>

</div><p></p><hr><a id="method-UrlScraper-clone"></a><div class="section">
<h3 id="method-clone-">Method <code>clone()</code><a class="anchor" aria-label="anchor" href="#method-clone-"></a></h3>
<p>The objects of this class are cloneable with this method.</p><div class="section">
<h4 id="usage-10">Usage<a class="anchor" aria-label="anchor" href="#usage-10"></a></h4>
<p></p><div class="r"><div class="sourceCode"><pre><code><span><span class="va">UrlScraper</span><span class="op">$</span><span class="fu">clone</span><span class="op">(</span>deep <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span></code></pre></div><p></p></div>
</div>

<div class="section">
<h4 id="arguments-7">Arguments<a class="anchor" aria-label="anchor" href="#arguments-7"></a></h4>
<p></p><div class="arguments"><dl><dt><code>deep</code></dt>
<dd><p>Whether to make a deep clone.</p></dd>


</dl><p></p></div>
</div>

</div>

    </div>

    <div class="section level2">
    <h2 id="ref-examples">Examples<a class="anchor" aria-label="anchor" href="#ref-examples"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span class="r-in"><span><span class="kw">if</span> <span class="op">(</span><span class="cn">FALSE</span><span class="op">)</span> <span class="op">{</span> <span class="co"># \dontrun{</span></span></span>
<span class="r-in"><span><span class="co"># Create a default configuration object</span></span></span>
<span class="r-in"><span><span class="va">cfg</span> <span class="op">&lt;-</span> <span class="fu"><a href="paramsScraper.html">paramsScraper</a></span><span class="op">(</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Example Selenium settings</span></span></span>
<span class="r-in"><span><span class="va">cfg</span><span class="op">$</span><span class="fu">set</span><span class="op">(</span><span class="st">"selenium$host"</span>, <span class="st">"localhost"</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">cfg</span><span class="op">$</span><span class="fu">set</span><span class="op">(</span><span class="st">"selenium$workers"</span>, <span class="fl">2</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">cfg</span><span class="op">$</span><span class="fu">show_config</span><span class="op">(</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Initialize the scraper</span></span></span>
<span class="r-in"><span><span class="va">scraper</span> <span class="op">&lt;-</span> <span class="va">UrlScraper</span><span class="op">$</span><span class="fu">new</span><span class="op">(</span>config <span class="op">=</span> <span class="va">cfg</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Start scraping remaining URLs</span></span></span>
<span class="r-in"><span><span class="va">scraper</span><span class="op">$</span><span class="fu">scrape</span><span class="op">(</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Retrieve results as a data.table</span></span></span>
<span class="r-in"><span><span class="va">results_dt</span> <span class="op">&lt;-</span> <span class="va">scraper</span><span class="op">$</span><span class="fu">results</span><span class="op">(</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Retrieve logs and links</span></span></span>
<span class="r-in"><span><span class="va">logs_dt</span>  <span class="op">&lt;-</span> <span class="va">scraper</span><span class="op">$</span><span class="fu">logs</span><span class="op">(</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">links_dt</span> <span class="op">&lt;-</span> <span class="va">scraper</span><span class="op">$</span><span class="fu">links</span><span class="op">(</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Add new URLs to be scraped (only those not already in the DB)</span></span></span>
<span class="r-in"><span><span class="va">scraper</span><span class="op">$</span><span class="fu">update_urls</span><span class="op">(</span>urls <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"https://example.com/"</span><span class="op">)</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Force adding URLs (ignores duplicates against already scraped ones)</span></span></span>
<span class="r-in"><span><span class="va">scraper</span><span class="op">$</span><span class="fu">update_urls</span><span class="op">(</span>urls <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"https://example.com/"</span><span class="op">)</span>, force <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Regex extraction from scraped HTML</span></span></span>
<span class="r-in"><span><span class="va">emails_dt</span> <span class="op">&lt;-</span> <span class="va">scraper</span><span class="op">$</span><span class="fu">regex_extract</span><span class="op">(</span></span></span>
<span class="r-in"><span>  pattern      <span class="op">=</span> <span class="st">"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}"</span>,</span></span>
<span class="r-in"><span>  filter_links <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"contact"</span>, <span class="st">"imprint"</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Stop ongoing workers after they finish the current URL</span></span></span>
<span class="r-in"><span><span class="va">scraper</span><span class="op">$</span><span class="kw">stop</span><span class="op">(</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Clean up resources</span></span></span>
<span class="r-in"><span><span class="va">scraper</span><span class="op">$</span><span class="fu">close</span><span class="op">(</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="op">}</span> <span class="co"># }</span></span></span>
</code></pre></div>
    </div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Statistics Austria</p>
</div>

<div class="pkgdown-footer-right">
  <p>Built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a>.</p>
</div>

    </footer></div>





  </body></html>

